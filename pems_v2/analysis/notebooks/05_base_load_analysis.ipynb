{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Load Analysis\n",
    "\n",
    "## Objectives:\n",
    "1. Separate base load from total energy consumption\n",
    "2. Identify consumption patterns (weekday vs weekend, seasonal, time-of-day)\n",
    "3. Model selection and validation for load prediction\n",
    "4. Anomaly detection in energy consumption\n",
    "5. Load clustering and classification\n",
    "6. Create features for load forecasting models\n",
    "\n",
    "## Key Analyses:\n",
    "- Base load extraction and decomposition\n",
    "- Consumption pattern analysis by time period\n",
    "- Load forecasting model development\n",
    "- Anomaly detection and outlier identification\n",
    "- Load clustering (typical vs atypical days)\n",
    "- Peak demand analysis and load balancing opportunities\n",
    "- Energy efficiency metrics and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport sys\nimport os\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport asyncio\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Add pems_v2 directory to path for imports\nsys.path.append(str(Path('../pems_v2').resolve()))\n\n# Import project modules\nfrom analysis.core.data_extraction import DataExtractor\nfrom analysis.analyzers.base_load_analysis import BaseLoadAnalyzer\nfrom config.settings import PEMSSettings\n\n# Set up plotting style\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette('husl')\n%matplotlib inline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Energy Consumption Data\n",
    "\n",
    "Load total energy consumption and related data for base load analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize settings and extractors\n",
    "settings = PEMSSettings()\n",
    "extractor = DataExtractor(settings)\n",
    "base_load_analyzer = BaseLoadAnalyzer()\n",
    "\n",
    "# Define analysis period (last 120 days for comprehensive pattern analysis)\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=120)\n",
    "\n",
    "print(f\"Analysis period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract consumption and related data\n",
    "async def load_consumption_data():\n",
    "    \"\"\"Load consumption and related data for base load analysis.\"\"\"\n",
    "    print(\"Loading energy consumption data...\")\n",
    "    consumption_data = await extractor.extract_energy_consumption(start_date, end_date)\n",
    "    \n",
    "    print(\"Loading PV production data...\")\n",
    "    pv_data = await extractor.extract_pv_data(start_date, end_date)\n",
    "    \n",
    "    print(\"Loading room temperature data...\")\n",
    "    room_data = await extractor.extract_room_temperatures(start_date, end_date)\n",
    "    \n",
    "    print(\"Loading heating relay data...\")\n",
    "    relay_data = await extractor.extract_relay_states(start_date, end_date)\n",
    "    \n",
    "    print(\"Loading weather data...\")\n",
    "    weather_data = await extractor.extract_weather_data(start_date, end_date)\n",
    "    \n",
    "    print(\"Loading EV charging data...\")\n",
    "    try:\n",
    "        ev_data = await extractor.extract_ev_data(start_date, end_date)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load EV data: {e}\")\n",
    "        ev_data = pd.DataFrame()\n",
    "    \n",
    "    print(\"Loading battery data...\")\n",
    "    try:\n",
    "        battery_data = await extractor.extract_battery_data(start_date, end_date)\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Could not load battery data: {e}\")\n",
    "        battery_data = pd.DataFrame()\n",
    "    \n",
    "    return consumption_data, pv_data, room_data, relay_data, weather_data, ev_data, battery_data\n",
    "\n",
    "# Load data\n",
    "consumption_data, pv_data, room_data, relay_data, weather_data, ev_data, battery_data = await load_consumption_data()\n",
    "\n",
    "print(f\"\\nData loaded:\")\n",
    "print(f\"  Consumption records: {len(consumption_data)}\")\n",
    "print(f\"  PV records: {len(pv_data)}\")\n",
    "print(f\"  Room data: {len(room_data)} rooms\")\n",
    "print(f\"  Relay data: {len(relay_data)} rooms\")\n",
    "print(f\"  Weather records: {len(weather_data)}\")\n",
    "print(f\"  EV records: {len(ev_data)}\")\n",
    "print(f\"  Battery records: {len(battery_data)}\")\n",
    "\n",
    "# Display consumption data columns\n",
    "if not consumption_data.empty:\n",
    "    print(f\"\\nConsumption data columns: {list(consumption_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation and Base Load Extraction\n",
    "\n",
    "Prepare consumption data and extract base load components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare consumption data for analysis\n",
    "if not consumption_data.empty:\n",
    "    # Resample to hourly data for cleaner analysis\n",
    "    consumption_hourly = consumption_data.resample('H').mean()\n",
    "    \n",
    "    # Identify total consumption column (look for power or energy columns)\n",
    "    power_columns = [col for col in consumption_hourly.columns if 'power' in col.lower()]\n",
    "    energy_columns = [col for col in consumption_hourly.columns if 'energy' in col.lower()]\n",
    "    \n",
    "    print(f\"Power columns found: {power_columns}\")\n",
    "    print(f\"Energy columns found: {energy_columns}\")\n",
    "    \n",
    "    # Use the first power column as total consumption, or create one\n",
    "    if power_columns:\n",
    "        total_power_col = power_columns[0]\n",
    "        consumption_hourly['total_power'] = consumption_hourly[total_power_col]\n",
    "    else:\n",
    "        # If no power columns, try to derive from PV data\n",
    "        if not pv_data.empty and 'ACPowerToUser' in pv_data.columns:\n",
    "            pv_hourly = pv_data['ACPowerToUser'].resample('H').mean()\n",
    "            consumption_hourly = pd.merge(consumption_hourly, pv_hourly.to_frame('total_power'), \n",
    "                                        left_index=True, right_index=True, how='outer')\n",
    "        else:\n",
    "            print(\"Warning: No suitable consumption data found\")\n",
    "            consumption_hourly = pd.DataFrame()\n",
    "    \n",
    "    if not consumption_hourly.empty and 'total_power' in consumption_hourly.columns:\n",
    "        # Remove negative values and outliers\n",
    "        consumption_hourly['total_power'] = consumption_hourly['total_power'].clip(lower=0)\n",
    "        \n",
    "        # Remove extreme outliers (above 99.5 percentile)\n",
    "        upper_limit = consumption_hourly['total_power'].quantile(0.995)\n",
    "        consumption_hourly['total_power'] = consumption_hourly['total_power'].clip(upper=upper_limit)\n",
    "        \n",
    "        # Add time features\n",
    "        consumption_hourly['hour'] = consumption_hourly.index.hour\n",
    "        consumption_hourly['day_of_week'] = consumption_hourly.index.dayofweek\n",
    "        consumption_hourly['month'] = consumption_hourly.index.month\n",
    "        consumption_hourly['is_weekend'] = consumption_hourly['day_of_week'].isin([5, 6])\n",
    "        consumption_hourly['is_working_hours'] = (\n",
    "            (consumption_hourly['hour'] >= 8) & \n",
    "            (consumption_hourly['hour'] <= 18) & \n",
    "            (~consumption_hourly['is_weekend'])\n",
    "        )\n",
    "        \n",
    "        print(f\"\\nPrepared consumption data: {len(consumption_hourly)} hours\")\n",
    "        print(f\"Average consumption: {consumption_hourly['total_power'].mean():.1f} W\")\n",
    "        print(f\"Peak consumption: {consumption_hourly['total_power'].max():.1f} W\")\n",
    "        print(f\"Minimum consumption: {consumption_hourly['total_power'].min():.1f} W\")\n",
    "    else:\n",
    "        print(\"Error: Could not prepare consumption data\")\n",
    "        consumption_hourly = pd.DataFrame()\n",
    "else:\n",
    "    print(\"Error: No consumption data available\")\n",
    "    consumption_hourly = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract base load using multiple methods\n",
    "def extract_base_load(consumption_data, method='percentile'):\n",
    "    \"\"\"Extract base load using different methods.\"\"\"\n",
    "    \n",
    "    if consumption_data.empty or 'total_power' not in consumption_data.columns:\n",
    "        return None\n",
    "    \n",
    "    power_series = consumption_data['total_power'].dropna()\n",
    "    \n",
    "    base_load_methods = {}\n",
    "    \n",
    "    # Method 1: Percentile approach (10th percentile)\n",
    "    base_load_methods['percentile_10'] = power_series.quantile(0.10)\n",
    "    base_load_methods['percentile_5'] = power_series.quantile(0.05)\n",
    "    \n",
    "    # Method 2: Minimum during night hours (2-5 AM)\n",
    "    night_hours = consumption_data[(consumption_data['hour'] >= 2) & (consumption_data['hour'] <= 5)]\n",
    "    if not night_hours.empty:\n",
    "        base_load_methods['night_minimum'] = night_hours['total_power'].min()\n",
    "        base_load_methods['night_median'] = night_hours['total_power'].median()\n",
    "    \n",
    "    # Method 3: Rolling minimum (24-hour window)\n",
    "    rolling_min = power_series.rolling(window=24, center=True).min()\n",
    "    base_load_methods['rolling_min_mean'] = rolling_min.mean()\n",
    "    base_load_methods['rolling_min_median'] = rolling_min.median()\n",
    "    \n",
    "    # Method 4: Statistical decomposition (if enough data)\n",
    "    if len(power_series) > 168:  # Need at least a week of data\n",
    "        try:\n",
    "            # Use STL decomposition\n",
    "            stl = STL(power_series.interpolate(), seasonal=13)  # Weekly seasonality\n",
    "            result = stl.fit()\n",
    "            trend_component = result.trend\n",
    "            base_load_methods['stl_trend_min'] = trend_component.min()\n",
    "            base_load_methods['stl_trend_p10'] = trend_component.quantile(0.10)\n",
    "        except Exception as e:\n",
    "            print(f\"STL decomposition failed: {e}\")\n",
    "    \n",
    "    return base_load_methods\n",
    "\n",
    "# Extract base load\n",
    "if not consumption_hourly.empty:\n",
    "    base_load_estimates = extract_base_load(consumption_hourly)\n",
    "    \n",
    "    if base_load_estimates:\n",
    "        print(\"\\nBase Load Estimation Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        for method, value in base_load_estimates.items():\n",
    "            print(f\"{method:20s}: {value:6.1f} W\")\n",
    "        \n",
    "        # Select the most conservative estimate (night median)\n",
    "        if 'night_median' in base_load_estimates:\n",
    "            base_load_estimate = base_load_estimates['night_median']\n",
    "        else:\n",
    "            base_load_estimate = base_load_estimates['percentile_10']\n",
    "        \n",
    "        # Calculate variable load\n",
    "        consumption_hourly['base_load'] = base_load_estimate\n",
    "        consumption_hourly['variable_load'] = consumption_hourly['total_power'] - base_load_estimate\n",
    "        consumption_hourly['variable_load'] = consumption_hourly['variable_load'].clip(lower=0)\n",
    "        \n",
    "        print(f\"\\nSelected base load estimate: {base_load_estimate:.1f} W\")\n",
    "        print(f\"Average variable load: {consumption_hourly['variable_load'].mean():.1f} W\")\n",
    "        print(f\"Base load percentage: {base_load_estimate/consumption_hourly['total_power'].mean()*100:.1f}%\")\n",
    "    else:\n",
    "        print(\"Error: Could not estimate base load\")\n",
    "else:\n",
    "    print(\"Error: No consumption data for base load extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consumption Pattern Analysis\n",
    "\n",
    "Analyze consumption patterns by time period and identify typical behaviors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze consumption patterns\n",
    "if not consumption_hourly.empty and 'total_power' in consumption_hourly.columns:\n",
    "    # Daily patterns\n",
    "    daily_patterns = {\n",
    "        'weekday': consumption_hourly[~consumption_hourly['is_weekend']].groupby('hour')['total_power'].agg(['mean', 'std', 'min', 'max']),\n",
    "        'weekend': consumption_hourly[consumption_hourly['is_weekend']].groupby('hour')['total_power'].agg(['mean', 'std', 'min', 'max']),\n",
    "        'all_days': consumption_hourly.groupby('hour')['total_power'].agg(['mean', 'std', 'min', 'max'])\n",
    "    }\n",
    "    \n",
    "    # Weekly patterns\n",
    "    weekly_pattern = consumption_hourly.groupby('day_of_week')['total_power'].agg(['mean', 'std', 'min', 'max'])\n",
    "    \n",
    "    # Monthly patterns\n",
    "    monthly_pattern = consumption_hourly.groupby('month')['total_power'].agg(['mean', 'std', 'min', 'max'])\n",
    "    \n",
    "    print(\"\\nConsumption Pattern Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Find peak consumption hours\n",
    "    peak_hour_weekday = daily_patterns['weekday']['mean'].idxmax()\n",
    "    peak_power_weekday = daily_patterns['weekday']['mean'].max()\n",
    "    \n",
    "    peak_hour_weekend = daily_patterns['weekend']['mean'].idxmax()\n",
    "    peak_power_weekend = daily_patterns['weekend']['mean'].max()\n",
    "    \n",
    "    print(f\"Peak consumption - Weekdays: {peak_hour_weekday:02d}:00 ({peak_power_weekday:.0f}W)\")\n",
    "    print(f\"Peak consumption - Weekends: {peak_hour_weekend:02d}:00 ({peak_power_weekend:.0f}W)\")\n",
    "    \n",
    "    # Find lowest consumption hours\n",
    "    low_hour_weekday = daily_patterns['weekday']['mean'].idxmin()\n",
    "    low_power_weekday = daily_patterns['weekday']['mean'].min()\n",
    "    \n",
    "    low_hour_weekend = daily_patterns['weekend']['mean'].idxmin()\n",
    "    low_power_weekend = daily_patterns['weekend']['mean'].min()\n",
    "    \n",
    "    print(f\"Low consumption - Weekdays: {low_hour_weekday:02d}:00 ({low_power_weekday:.0f}W)\")\n",
    "    print(f\"Low consumption - Weekends: {low_hour_weekend:02d}:00 ({low_power_weekend:.0f}W)\")\n",
    "    \n",
    "    # Calculate daily load factor\n",
    "    daily_load_factor = daily_patterns['all_days']['mean'].mean() / daily_patterns['all_days']['max'].max()\n",
    "    print(f\"\\nDaily load factor: {daily_load_factor:.3f}\")\n",
    "    \n",
    "    # Weekend vs weekday comparison\n",
    "    weekend_avg = daily_patterns['weekend']['mean'].mean()\n",
    "    weekday_avg = daily_patterns['weekday']['mean'].mean()\n",
    "    weekend_difference = (weekend_avg - weekday_avg) / weekday_avg * 100\n",
    "    \n",
    "    print(f\"Weekend vs weekday difference: {weekend_difference:+.1f}%\")\n",
    "else:\n",
    "    print(\"No consumption data available for pattern analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize consumption patterns\n",
    "if 'daily_patterns' in locals():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    \n",
    "    # Daily patterns comparison\n",
    "    daily_patterns['weekday']['mean'].plot(ax=axes[0,0], label='Weekday', linewidth=2, marker='o')\n",
    "    daily_patterns['weekend']['mean'].plot(ax=axes[0,0], label='Weekend', linewidth=2, marker='s')\n",
    "    axes[0,0].fill_between(daily_patterns['weekday'].index, \n",
    "                          daily_patterns['weekday']['mean'] - daily_patterns['weekday']['std'],\n",
    "                          daily_patterns['weekday']['mean'] + daily_patterns['weekday']['std'],\n",
    "                          alpha=0.3)\n",
    "    axes[0,0].set_title('Daily Consumption Patterns')\n",
    "    axes[0,0].set_xlabel('Hour of Day')\n",
    "    axes[0,0].set_ylabel('Power (W)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Weekly pattern\n",
    "    weekly_pattern['mean'].plot(kind='bar', ax=axes[0,1], color='orange', alpha=0.8)\n",
    "    axes[0,1].set_title('Weekly Consumption Pattern')\n",
    "    axes[0,1].set_xlabel('Day of Week (0=Monday)')\n",
    "    axes[0,1].set_ylabel('Average Power (W)')\n",
    "    axes[0,1].set_xticklabels(['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'], rotation=0)\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Monthly pattern\n",
    "    monthly_pattern['mean'].plot(kind='bar', ax=axes[0,2], color='green', alpha=0.8)\n",
    "    axes[0,2].set_title('Monthly Consumption Pattern')\n",
    "    axes[0,2].set_xlabel('Month')\n",
    "    axes[0,2].set_ylabel('Average Power (W)')\n",
    "    axes[0,2].set_xticklabels(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun',\n",
    "                              'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], rotation=45)\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Base load vs variable load\n",
    "    if 'variable_load' in consumption_hourly.columns:\n",
    "        daily_base = consumption_hourly.groupby('hour')['base_load'].mean()\n",
    "        daily_variable = consumption_hourly.groupby('hour')['variable_load'].mean()\n",
    "        \n",
    "        daily_base.plot(ax=axes[1,0], label='Base Load', linewidth=2, color='red')\n",
    "        daily_variable.plot(ax=axes[1,0], label='Variable Load', linewidth=2, color='blue')\n",
    "        axes[1,0].set_title('Base Load vs Variable Load')\n",
    "        axes[1,0].set_xlabel('Hour of Day')\n",
    "        axes[1,0].set_ylabel('Power (W)')\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Load duration curve\n",
    "    sorted_loads = consumption_hourly['total_power'].sort_values(ascending=False)\n",
    "    hours = np.arange(len(sorted_loads)) / len(sorted_loads) * 100\n",
    "    \n",
    "    axes[1,1].plot(hours, sorted_loads, linewidth=2, color='purple')\n",
    "    axes[1,1].axhline(y=consumption_hourly['total_power'].mean(), color='red', \n",
    "                     linestyle='--', label=f'Average: {consumption_hourly[\"total_power\"].mean():.0f}W')\n",
    "    if 'base_load_estimate' in locals():\n",
    "        axes[1,1].axhline(y=base_load_estimate, color='green', \n",
    "                         linestyle='--', label=f'Base Load: {base_load_estimate:.0f}W')\n",
    "    axes[1,1].set_title('Load Duration Curve')\n",
    "    axes[1,1].set_xlabel('Time (%)')\n",
    "    axes[1,1].set_ylabel('Power (W)')\n",
    "    axes[1,1].legend()\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Consumption distribution\n",
    "    consumption_hourly['total_power'].hist(bins=50, ax=axes[1,2], alpha=0.7, color='skyblue')\n",
    "    axes[1,2].axvline(consumption_hourly['total_power'].mean(), color='red', \n",
    "                     linestyle='--', label=f'Mean: {consumption_hourly[\"total_power\"].mean():.0f}W')\n",
    "    axes[1,2].axvline(consumption_hourly['total_power'].median(), color='orange', \n",
    "                     linestyle='--', label=f'Median: {consumption_hourly[\"total_power\"].median():.0f}W')\n",
    "    axes[1,2].set_title('Consumption Distribution')\n",
    "    axes[1,2].set_xlabel('Power (W)')\n",
    "    axes[1,2].set_ylabel('Frequency')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Seasonal Decomposition\n",
    "\n",
    "Decompose consumption data into trend, seasonal, and residual components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform seasonal decomposition\n",
    "if not consumption_hourly.empty and len(consumption_hourly) > 168:  # Need at least a week\n",
    "    power_series = consumption_hourly['total_power'].interpolate()\n",
    "    \n",
    "    try:\n",
    "        # STL decomposition with multiple seasonalities\n",
    "        stl_daily = STL(power_series, seasonal=25, period=24)  # Daily seasonality\n",
    "        result_daily = stl_daily.fit()\n",
    "        \n",
    "        # Extract components\n",
    "        trend = result_daily.trend\n",
    "        seasonal = result_daily.seasonal\n",
    "        residual = result_daily.resid\n",
    "        \n",
    "        # Calculate component statistics\n",
    "        trend_strength = 1 - np.var(residual + seasonal) / np.var(power_series)\n",
    "        seasonal_strength = 1 - np.var(residual + trend) / np.var(power_series)\n",
    "        \n",
    "        print(\"\\nSeasonal Decomposition Results:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"Trend strength: {trend_strength:.3f}\")\n",
    "        print(f\"Seasonal strength: {seasonal_strength:.3f}\")\n",
    "        print(f\"Residual variance: {np.var(residual):.1f}\")\n",
    "        \n",
    "        # Visualize decomposition\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(15, 12), sharex=True)\n",
    "        \n",
    "        # Original series\n",
    "        power_series.plot(ax=axes[0], title='Original Consumption', color='blue')\n",
    "        axes[0].set_ylabel('Power (W)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Trend\n",
    "        trend.plot(ax=axes[1], title='Trend Component', color='red')\n",
    "        axes[1].set_ylabel('Power (W)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Seasonal\n",
    "        seasonal.plot(ax=axes[2], title='Seasonal Component (Daily)', color='green')\n",
    "        axes[2].set_ylabel('Power (W)')\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residual\n",
    "        residual.plot(ax=axes[3], title='Residual Component', color='orange')\n",
    "        axes[3].set_ylabel('Power (W)')\n",
    "        axes[3].set_xlabel('Date')\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze residuals for anomalies\n",
    "        residual_std = residual.std()\n",
    "        anomaly_threshold = 3 * residual_std\n",
    "        anomalies = residual[abs(residual) > anomaly_threshold]\n",
    "        \n",
    "        print(f\"\\nAnomaly Detection (3-sigma rule):\")\n",
    "        print(f\"Residual standard deviation: {residual_std:.1f} W\")\n",
    "        print(f\"Anomaly threshold: ±{anomaly_threshold:.1f} W\")\n",
    "        print(f\"Anomalies detected: {len(anomalies)} ({len(anomalies)/len(residual)*100:.2f}%)\")\n",
    "        \n",
    "        # Store decomposition results\n",
    "        decomposition_results = {\n",
    "            'trend': trend,\n",
    "            'seasonal': seasonal,\n",
    "            'residual': residual,\n",
    "            'trend_strength': trend_strength,\n",
    "            'seasonal_strength': seasonal_strength,\n",
    "            'anomalies': anomalies\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Seasonal decomposition failed: {e}\")\n",
    "        decomposition_results = None\n",
    "else:\n",
    "    print(\"Insufficient data for seasonal decomposition\")\n",
    "    decomposition_results = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Forecasting Model Development\n",
    "\n",
    "Develop and validate load forecasting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for load forecasting\n",
    "if not consumption_hourly.empty:\n",
    "    # Create feature matrix\n",
    "    features_df = consumption_hourly.copy()\n",
    "    \n",
    "    # Add lag features\n",
    "    for lag in [1, 2, 3, 24, 48, 168]:  # 1h, 2h, 3h, 1d, 2d, 1w\n",
    "        features_df[f'power_lag_{lag}h'] = features_df['total_power'].shift(lag)\n",
    "    \n",
    "    # Add rolling statistics\n",
    "    for window in [6, 12, 24]:  # 6h, 12h, 24h windows\n",
    "        features_df[f'power_rolling_mean_{window}h'] = features_df['total_power'].rolling(window).mean()\n",
    "        features_df[f'power_rolling_std_{window}h'] = features_df['total_power'].rolling(window).std()\n",
    "    \n",
    "    # Add cyclical time features\n",
    "    features_df['hour_sin'] = np.sin(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['hour_cos'] = np.cos(2 * np.pi * features_df['hour'] / 24)\n",
    "    features_df['day_sin'] = np.sin(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    features_df['day_cos'] = np.cos(2 * np.pi * features_df['day_of_week'] / 7)\n",
    "    features_df['month_sin'] = np.sin(2 * np.pi * features_df['month'] / 12)\n",
    "    features_df['month_cos'] = np.cos(2 * np.pi * features_df['month'] / 12)\n",
    "    \n",
    "    # Add weather features if available\n",
    "    if not weather_data.empty:\n",
    "        weather_hourly = weather_data.resample('H').mean()\n",
    "        weather_features = ['temperature_2m', 'cloudcover', 'shortwave_radiation']\n",
    "        available_weather = [col for col in weather_features if col in weather_hourly.columns]\n",
    "        \n",
    "        if available_weather:\n",
    "            features_df = pd.merge(features_df, weather_hourly[available_weather], \n",
    "                                 left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Add heating load estimate (if relay data available)\n",
    "    if relay_data:\n",
    "        # Estimate heating power consumption\n",
    "        total_heating_power = 0\n",
    "        for room, relay_df in relay_data.items():\n",
    "            if not relay_df.empty:\n",
    "                # Estimate room power (using simple mapping)\n",
    "                room_power_map = {\n",
    "                    'obyvacka': 2000, 'kuchyn': 1500, 'loznice': 1500,\n",
    "                    'detsky_pokoj': 1000, 'koupelna': 800, 'pracovna': 1200\n",
    "                }\n",
    "                room_power = room_power_map.get(room, 1000)\n",
    "                \n",
    "                relay_hourly = relay_df['value'].resample('H').mean()\n",
    "                heating_power = relay_hourly * room_power\n",
    "                total_heating_power += heating_power\n",
    "        \n",
    "        if isinstance(total_heating_power, pd.Series) and not total_heating_power.empty:\n",
    "            features_df = pd.merge(features_df, total_heating_power.to_frame('heating_power'), \n",
    "                                 left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    # Remove rows with missing values\n",
    "    features_df = features_df.dropna()\n",
    "    \n",
    "    print(f\"\\nFeature Engineering Completed:\")\n",
    "    print(f\"Total features: {len(features_df.columns)}\")\n",
    "    print(f\"Training samples: {len(features_df)}\")\n",
    "    \n",
    "    # Select features for modeling\n",
    "    feature_columns = [col for col in features_df.columns \n",
    "                      if col not in ['total_power', 'base_load', 'variable_load']]\n",
    "    \n",
    "    X = features_df[feature_columns]\n",
    "    y = features_df['total_power']\n",
    "    \n",
    "    print(f\"Model features: {len(feature_columns)}\")\n",
    "    print(f\"Sample feature names: {feature_columns[:10]}\")\n",
    "else:\n",
    "    print(\"No consumption data available for forecasting model\")\n",
    "    features_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and validate forecasting models\n",
    "if not features_df.empty and len(features_df) > 100:\n",
    "    # Prepare data for time series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # Initialize models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    }\n",
    "    \n",
    "    model_results = {}\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        cv_scores = {'mae': [], 'rmse': [], 'r2': []}\n",
    "        \n",
    "        for train_idx, test_idx in tscv.split(X_scaled):\n",
    "            X_train, X_test = X_scaled[train_idx], X_scaled[test_idx]\n",
    "            y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Train model\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Make predictions\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Calculate metrics\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            cv_scores['mae'].append(mae)\n",
    "            cv_scores['rmse'].append(rmse)\n",
    "            cv_scores['r2'].append(r2)\n",
    "        \n",
    "        # Store average results\n",
    "        model_results[name] = {\n",
    "            'mae_mean': np.mean(cv_scores['mae']),\n",
    "            'mae_std': np.std(cv_scores['mae']),\n",
    "            'rmse_mean': np.mean(cv_scores['rmse']),\n",
    "            'rmse_std': np.std(cv_scores['rmse']),\n",
    "            'r2_mean': np.mean(cv_scores['r2']),\n",
    "            'r2_std': np.std(cv_scores['r2'])\n",
    "        }\n",
    "    \n",
    "    # Display model performance\n",
    "    print(\"\\nModel Performance (Cross-Validation):\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"{'Model':20s} {'MAE (W)':12s} {'RMSE (W)':12s} {'R²':12s}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for name, results in model_results.items():\n",
    "        mae_str = f\"{results['mae_mean']:.1f}±{results['mae_std']:.1f}\"\n",
    "        rmse_str = f\"{results['rmse_mean']:.1f}±{results['rmse_std']:.1f}\"\n",
    "        r2_str = f\"{results['r2_mean']:.3f}±{results['r2_std']:.3f}\"\n",
    "        \n",
    "        print(f\"{name:20s} {mae_str:12s} {rmse_str:12s} {r2_str:12s}\")\n",
    "    \n",
    "    # Train final model on all data\n",
    "    best_model_name = max(model_results.keys(), key=lambda x: model_results[x]['r2_mean'])\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X_scaled, y)\n",
    "    \n",
    "    # Feature importance (for Random Forest)\n",
    "    if best_model_name == 'Random Forest':\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_columns,\n",
    "            'importance': best_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 10 Important Features ({best_model_name}):\")\n",
    "        print(\"-\" * 50)\n",
    "        for _, row in feature_importance.head(10).iterrows():\n",
    "            print(f\"{row['feature']:30s}: {row['importance']:.4f}\")\n",
    "    \n",
    "    # Generate forecast for next 24 hours\n",
    "    if len(features_df) >= 24:\n",
    "        # Use last 24 hours for forecast\n",
    "        last_features = X_scaled[-24:]\n",
    "        forecast_24h = best_model.predict(last_features)\n",
    "        \n",
    "        # Create forecast dataframe\n",
    "        forecast_index = pd.date_range(start=features_df.index[-24], periods=24, freq='H')\n",
    "        forecast_df = pd.DataFrame({\n",
    "            'actual': y.iloc[-24:].values,\n",
    "            'forecast': forecast_24h\n",
    "        }, index=forecast_index)\n",
    "        \n",
    "        # Calculate forecast accuracy\n",
    "        forecast_mae = mean_absolute_error(forecast_df['actual'], forecast_df['forecast'])\n",
    "        forecast_rmse = np.sqrt(mean_squared_error(forecast_df['actual'], forecast_df['forecast']))\n",
    "        \n",
    "        print(f\"\\n24-hour Forecast Accuracy:\")\n",
    "        print(f\"MAE: {forecast_mae:.1f} W\")\n",
    "        print(f\"RMSE: {forecast_rmse:.1f} W\")\n",
    "        \n",
    "        # Plot forecast\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(forecast_df.index, forecast_df['actual'], 'b-', linewidth=2, label='Actual')\n",
    "        plt.plot(forecast_df.index, forecast_df['forecast'], 'r--', linewidth=2, label='Forecast')\n",
    "        plt.fill_between(forecast_df.index, \n",
    "                        forecast_df['forecast'] - forecast_rmse,\n",
    "                        forecast_df['forecast'] + forecast_rmse,\n",
    "                        alpha=0.3, color='red', label='±RMSE band')\n",
    "        plt.title(f'24-hour Load Forecast ({best_model_name})')\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Power (W)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"Insufficient data for load forecasting model development\")\n",
    "    model_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Anomaly Detection\n",
    "\n",
    "Detect anomalies and unusual consumption patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection using multiple methods\n",
    "if not consumption_hourly.empty:\n",
    "    \n",
    "    # Method 1: Statistical outliers (Z-score)\n",
    "    power_series = consumption_hourly['total_power']\n",
    "    z_scores = np.abs(stats.zscore(power_series))\n",
    "    z_threshold = 3\n",
    "    z_anomalies = power_series[z_scores > z_threshold]\n",
    "    \n",
    "    # Method 2: Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "    power_reshaped = power_series.values.reshape(-1, 1)\n",
    "    iso_outliers = iso_forest.fit_predict(power_reshaped)\n",
    "    iso_anomalies = power_series[iso_outliers == -1]\n",
    "    \n",
    "    # Method 3: Residual-based anomalies (if decomposition available)\n",
    "    residual_anomalies = pd.Series(dtype=float)\n",
    "    if 'decomposition_results' in locals() and decomposition_results:\n",
    "        residual_anomalies = decomposition_results['anomalies']\n",
    "    \n",
    "    # Method 4: Inter-quartile range (IQR)\n",
    "    Q1 = power_series.quantile(0.25)\n",
    "    Q3 = power_series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    iqr_lower = Q1 - 1.5 * IQR\n",
    "    iqr_upper = Q3 + 1.5 * IQR\n",
    "    iqr_anomalies = power_series[(power_series < iqr_lower) | (power_series > iqr_upper)]\n",
    "    \n",
    "    print(\"\\nAnomaly Detection Results:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(f\"Total data points: {len(power_series)}\")\n",
    "    print(f\"Z-score outliers (>3σ): {len(z_anomalies)} ({len(z_anomalies)/len(power_series)*100:.2f}%)\")\n",
    "    print(f\"Isolation Forest outliers: {len(iso_anomalies)} ({len(iso_anomalies)/len(power_series)*100:.2f}%)\")\n",
    "    if len(residual_anomalies) > 0:\n",
    "        print(f\"Residual-based outliers: {len(residual_anomalies)} ({len(residual_anomalies)/len(power_series)*100:.2f}%)\")\n",
    "    print(f\"IQR outliers: {len(iqr_anomalies)} ({len(iqr_anomalies)/len(power_series)*100:.2f}%)\")\n",
    "    \n",
    "    # Combine anomalies (union of all methods)\n",
    "    all_anomaly_indices = set()\n",
    "    all_anomaly_indices.update(z_anomalies.index)\n",
    "    all_anomaly_indices.update(iso_anomalies.index)\n",
    "    if len(residual_anomalies) > 0:\n",
    "        all_anomaly_indices.update(residual_anomalies.index)\n",
    "    all_anomaly_indices.update(iqr_anomalies.index)\n",
    "    \n",
    "    combined_anomalies = power_series.loc[list(all_anomaly_indices)]\n",
    "    \n",
    "    print(f\"\\nCombined anomalies: {len(combined_anomalies)} ({len(combined_anomalies)/len(power_series)*100:.2f}%)\")\n",
    "    \n",
    "    # Analyze anomaly patterns\n",
    "    if len(combined_anomalies) > 0:\n",
    "        anomaly_df = pd.DataFrame({\n",
    "            'power': combined_anomalies,\n",
    "            'hour': combined_anomalies.index.hour,\n",
    "            'day_of_week': combined_anomalies.index.dayofweek,\n",
    "            'month': combined_anomalies.index.month\n",
    "        })\n",
    "        \n",
    "        print(\"\\nAnomaly Pattern Analysis:\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Hour distribution\n",
    "        hour_dist = anomaly_df['hour'].value_counts().sort_index()\n",
    "        if len(hour_dist) > 0:\n",
    "            most_common_hour = hour_dist.idxmax()\n",
    "            print(f\"Most common anomaly hour: {most_common_hour:02d}:00 ({hour_dist[most_common_hour]} cases)\")\n",
    "        \n",
    "        # Day of week distribution\n",
    "        day_dist = anomaly_df['day_of_week'].value_counts().sort_index()\n",
    "        if len(day_dist) > 0:\n",
    "            days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "            most_common_day = day_dist.idxmax()\n",
    "            print(f\"Most common anomaly day: {days[most_common_day]} ({day_dist[most_common_day]} cases)\")\n",
    "        \n",
    "        # Power range analysis\n",
    "        high_anomalies = anomaly_df[anomaly_df['power'] > power_series.quantile(0.95)]\n",
    "        low_anomalies = anomaly_df[anomaly_df['power'] < power_series.quantile(0.05)]\n",
    "        \n",
    "        print(f\"High consumption anomalies: {len(high_anomalies)} (>{power_series.quantile(0.95):.0f}W)\")\n",
    "        print(f\"Low consumption anomalies: {len(low_anomalies)} (<{power_series.quantile(0.05):.0f}W)\")\n",
    "        \n",
    "        # Recent anomalies\n",
    "        recent_anomalies = combined_anomalies.tail(5)\n",
    "        if len(recent_anomalies) > 0:\n",
    "            print(f\"\\nRecent anomalies:\")\n",
    "            for timestamp, power in recent_anomalies.items():\n",
    "                print(f\"  {timestamp}: {power:.0f}W\")\n",
    "    \n",
    "    # Visualize anomalies\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Plot consumption with anomalies highlighted\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(power_series.index, power_series.values, 'b-', alpha=0.7, linewidth=1, label='Normal')\n",
    "    \n",
    "    if len(combined_anomalies) > 0:\n",
    "        plt.scatter(combined_anomalies.index, combined_anomalies.values, \n",
    "                   c='red', s=50, label=f'Anomalies ({len(combined_anomalies)})', zorder=5)\n",
    "    \n",
    "    plt.title('Consumption with Anomalies Highlighted')\n",
    "    plt.ylabel('Power (W)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot anomaly distribution by hour\n",
    "    plt.subplot(2, 1, 2)\n",
    "    if len(combined_anomalies) > 0:\n",
    "        hourly_anomalies = combined_anomalies.groupby(combined_anomalies.index.hour).size()\n",
    "        hourly_normal = power_series.groupby(power_series.index.hour).size()\n",
    "        \n",
    "        hours = range(24)\n",
    "        anomaly_counts = [hourly_anomalies.get(h, 0) for h in hours]\n",
    "        normal_counts = [hourly_normal.get(h, 0) for h in hours]\n",
    "        anomaly_rates = [a/(n+a)*100 if (n+a) > 0 else 0 for a, n in zip(anomaly_counts, normal_counts)]\n",
    "        \n",
    "        plt.bar(hours, anomaly_rates, alpha=0.7, color='red')\n",
    "        plt.title('Anomaly Rate by Hour of Day')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Anomaly Rate (%)')\n",
    "        plt.xticks(range(0, 24, 2))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Store anomaly results\n",
    "    anomaly_results = {\n",
    "        'z_score_anomalies': z_anomalies,\n",
    "        'isolation_forest_anomalies': iso_anomalies,\n",
    "        'iqr_anomalies': iqr_anomalies,\n",
    "        'combined_anomalies': combined_anomalies,\n",
    "        'anomaly_rate': len(combined_anomalies) / len(power_series) * 100\n",
    "    }\n",
    "    \n",
    "    if len(residual_anomalies) > 0:\n",
    "        anomaly_results['residual_anomalies'] = residual_anomalies\n",
    "        \n",
    "else:\n",
    "    print(\"No consumption data available for anomaly detection\")\n",
    "    anomaly_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Clustering Analysis\n",
    "\n",
    "Cluster consumption patterns to identify typical vs atypical days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustering analysis\n",
    "if not consumption_hourly.empty and len(consumption_hourly) >= 48:  # Need at least 2 days\n",
    "    \n",
    "    # Create daily load profiles\n",
    "    daily_profiles = consumption_hourly['total_power'].groupby(consumption_hourly.index.date).apply(\n",
    "        lambda x: x.reindex(pd.date_range(x.index.min().normalize(), \n",
    "                                         x.index.min().normalize() + pd.Timedelta(hours=23),\n",
    "                                         freq='H')).values\n",
    "    )\n",
    "    \n",
    "    # Filter complete days only (24 hours)\n",
    "    complete_days = daily_profiles[daily_profiles.apply(lambda x: len(x) == 24 and not np.any(np.isnan(x)))]\n",
    "    \n",
    "    if len(complete_days) >= 7:  # Need at least a week\n",
    "        # Convert to matrix for clustering\n",
    "        profile_matrix = np.vstack(complete_days.values)\n",
    "        \n",
    "        # Normalize profiles for clustering\n",
    "        scaler_cluster = StandardScaler()\n",
    "        profile_matrix_scaled = scaler_cluster.fit_transform(profile_matrix)\n",
    "        \n",
    "        # Determine optimal number of clusters using elbow method\n",
    "        max_clusters = min(8, len(complete_days) // 2)\n",
    "        inertias = []\n",
    "        \n",
    "        for k in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "            kmeans.fit(profile_matrix_scaled)\n",
    "            inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Find elbow point (simplified)\n",
    "        if len(inertias) >= 3:\n",
    "            diffs = np.diff(inertias)\n",
    "            optimal_k = np.argmin(diffs[1:]) + 3  # +3 because we start from k=2 and use second differences\n",
    "        else:\n",
    "            optimal_k = 3  # Default\n",
    "        \n",
    "        optimal_k = min(optimal_k, max_clusters)\n",
    "        \n",
    "        # Perform clustering with optimal k\n",
    "        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans_final.fit_predict(profile_matrix_scaled)\n",
    "        \n",
    "        # Analyze clusters\n",
    "        cluster_analysis = {}\n",
    "        \n",
    "        for cluster_id in range(optimal_k):\n",
    "            cluster_mask = cluster_labels == cluster_id\n",
    "            cluster_profiles = profile_matrix[cluster_mask]\n",
    "            cluster_dates = complete_days.index[cluster_mask]\n",
    "            \n",
    "            # Calculate cluster statistics\n",
    "            cluster_mean = cluster_profiles.mean(axis=0)\n",
    "            cluster_std = cluster_profiles.std(axis=0)\n",
    "            \n",
    "            # Analyze cluster characteristics\n",
    "            daily_total = cluster_profiles.sum(axis=1)\n",
    "            peak_hour = cluster_mean.argmax()\n",
    "            peak_power = cluster_mean.max()\n",
    "            min_power = cluster_mean.min()\n",
    "            \n",
    "            # Day type analysis\n",
    "            cluster_dates_dt = pd.to_datetime(cluster_dates)\n",
    "            weekdays = sum(cluster_dates_dt.weekday < 5)\n",
    "            weekends = sum(cluster_dates_dt.weekday >= 5)\n",
    "            \n",
    "            cluster_analysis[cluster_id] = {\n",
    "                'count': len(cluster_profiles),\n",
    "                'mean_profile': cluster_mean,\n",
    "                'std_profile': cluster_std,\n",
    "                'daily_total_mean': daily_total.mean(),\n",
    "                'daily_total_std': daily_total.std(),\n",
    "                'peak_hour': peak_hour,\n",
    "                'peak_power': peak_power,\n",
    "                'min_power': min_power,\n",
    "                'load_factor': min_power / peak_power,\n",
    "                'weekdays': weekdays,\n",
    "                'weekends': weekends,\n",
    "                'dates': cluster_dates.tolist()\n",
    "            }\n",
    "        \n",
    "        print(f\"\\nLoad Clustering Analysis ({optimal_k} clusters):\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{'Cluster':8s} {'Days':6s} {'Weekdays':9s} {'Weekends':9s} {'Peak Hour':10s} {'Peak (W)':9s} {'Load Factor':12s}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            print(f\"{cluster_id:8d} {analysis['count']:6d} {analysis['weekdays']:9d} {analysis['weekends']:9d} \"\n",
    "                 f\"{analysis['peak_hour']:9d}:00 {analysis['peak_power']:8.0f} {analysis['load_factor']:11.3f}\")\n",
    "        \n",
    "        # Visualize clusters\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        \n",
    "        # Plot cluster profiles\n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, optimal_k))\n",
    "        \n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            hours = range(24)\n",
    "            mean_profile = analysis['mean_profile']\n",
    "            std_profile = analysis['std_profile']\n",
    "            \n",
    "            axes[0,0].plot(hours, mean_profile, linewidth=2, color=colors[cluster_id], \n",
    "                          label=f'Cluster {cluster_id} ({analysis[\"count\"]} days)')\n",
    "            axes[0,0].fill_between(hours, mean_profile - std_profile, mean_profile + std_profile,\n",
    "                                  alpha=0.3, color=colors[cluster_id])\n",
    "        \n",
    "        axes[0,0].set_title('Daily Load Profile Clusters')\n",
    "        axes[0,0].set_xlabel('Hour of Day')\n",
    "        axes[0,0].set_ylabel('Power (W)')\n",
    "        axes[0,0].legend()\n",
    "        axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cluster size distribution\n",
    "        cluster_sizes = [analysis['count'] for analysis in cluster_analysis.values()]\n",
    "        cluster_names = [f'Cluster {i}' for i in range(optimal_k)]\n",
    "        \n",
    "        axes[0,1].bar(cluster_names, cluster_sizes, color=colors[:optimal_k], alpha=0.8)\n",
    "        axes[0,1].set_title('Cluster Size Distribution')\n",
    "        axes[0,1].set_ylabel('Number of Days')\n",
    "        axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Weekday vs weekend distribution\n",
    "        weekday_counts = [analysis['weekdays'] for analysis in cluster_analysis.values()]\n",
    "        weekend_counts = [analysis['weekends'] for analysis in cluster_analysis.values()]\n",
    "        \n",
    "        x = np.arange(optimal_k)\n",
    "        width = 0.35\n",
    "        \n",
    "        axes[1,0].bar(x - width/2, weekday_counts, width, label='Weekdays', alpha=0.8)\n",
    "        axes[1,0].bar(x + width/2, weekend_counts, width, label='Weekends', alpha=0.8)\n",
    "        axes[1,0].set_title('Day Type Distribution by Cluster')\n",
    "        axes[1,0].set_xlabel('Cluster')\n",
    "        axes[1,0].set_ylabel('Number of Days')\n",
    "        axes[1,0].set_xticks(x)\n",
    "        axes[1,0].set_xticklabels([f'C{i}' for i in range(optimal_k)])\n",
    "        axes[1,0].legend()\n",
    "        axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Daily total consumption by cluster\n",
    "        daily_totals = [analysis['daily_total_mean'] for analysis in cluster_analysis.values()]\n",
    "        daily_totals_std = [analysis['daily_total_std'] for analysis in cluster_analysis.values()]\n",
    "        \n",
    "        axes[1,1].bar(cluster_names, daily_totals, yerr=daily_totals_std, \n",
    "                     color=colors[:optimal_k], alpha=0.8, capsize=5)\n",
    "        axes[1,1].set_title('Daily Total Consumption by Cluster')\n",
    "        axes[1,1].set_ylabel('Daily Total (Wh)')\n",
    "        axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Identify cluster characteristics\n",
    "        print(f\"\\nCluster Characteristics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for cluster_id, analysis in cluster_analysis.items():\n",
    "            weekday_pct = analysis['weekdays'] / analysis['count'] * 100\n",
    "            \n",
    "            if weekday_pct > 80:\n",
    "                day_type = \"Weekday pattern\"\n",
    "            elif weekday_pct < 20:\n",
    "                day_type = \"Weekend pattern\"\n",
    "            else:\n",
    "                day_type = \"Mixed pattern\"\n",
    "            \n",
    "            peak_time = analysis['peak_hour']\n",
    "            if 6 <= peak_time <= 9:\n",
    "                peak_desc = \"Morning peak\"\n",
    "            elif 17 <= peak_time <= 21:\n",
    "                peak_desc = \"Evening peak\"\n",
    "            elif 10 <= peak_time <= 16:\n",
    "                peak_desc = \"Daytime peak\"\n",
    "            else:\n",
    "                peak_desc = \"Night peak\"\n",
    "            \n",
    "            load_factor = analysis['load_factor']\n",
    "            if load_factor > 0.7:\n",
    "                load_desc = \"High load factor (stable)\"\n",
    "            elif load_factor > 0.5:\n",
    "                load_desc = \"Medium load factor\"\n",
    "            else:\n",
    "                load_desc = \"Low load factor (variable)\"\n",
    "            \n",
    "            print(f\"Cluster {cluster_id}: {day_type}, {peak_desc}, {load_desc}\")\n",
    "        \n",
    "        # Store clustering results\n",
    "        clustering_results = {\n",
    "            'optimal_clusters': optimal_k,\n",
    "            'cluster_labels': cluster_labels,\n",
    "            'cluster_analysis': cluster_analysis,\n",
    "            'profile_matrix': profile_matrix,\n",
    "            'complete_days': complete_days\n",
    "        }\n",
    "        \n",
    "    else:\n",
    "        print(f\"Insufficient complete days for clustering ({len(complete_days)} < 7)\")\n",
    "        clustering_results = {}\n",
    "else:\n",
    "    print(\"Insufficient data for load clustering analysis\")\n",
    "    clustering_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Energy Efficiency Analysis\n",
    "\n",
    "Analyze energy efficiency metrics and benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy efficiency analysis\n",
    "if not consumption_hourly.empty:\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    efficiency_metrics = {}\n",
    "    \n",
    "    # Basic consumption statistics\n",
    "    total_period_hours = len(consumption_hourly)\n",
    "    total_energy_kwh = consumption_hourly['total_power'].sum() / 1000  # Convert Wh to kWh\n",
    "    avg_power_w = consumption_hourly['total_power'].mean()\n",
    "    peak_power_w = consumption_hourly['total_power'].max()\n",
    "    \n",
    "    # Load factor\n",
    "    load_factor = avg_power_w / peak_power_w\n",
    "    \n",
    "    # Capacity factor (assuming rated capacity)\n",
    "    assumed_max_capacity = 15000  # 15kW assumed maximum load\n",
    "    capacity_factor = avg_power_w / assumed_max_capacity\n",
    "    \n",
    "    efficiency_metrics['total_energy_kwh'] = total_energy_kwh\n",
    "    efficiency_metrics['avg_power_w'] = avg_power_w\n",
    "    efficiency_metrics['peak_power_w'] = peak_power_w\n",
    "    efficiency_metrics['load_factor'] = load_factor\n",
    "    efficiency_metrics['capacity_factor'] = capacity_factor\n",
    "    \n",
    "    # Daily efficiency metrics\n",
    "    daily_energy = consumption_hourly['total_power'].resample('D').sum() / 1000  # kWh per day\n",
    "    daily_peak = consumption_hourly['total_power'].resample('D').max()\n",
    "    daily_avg = consumption_hourly['total_power'].resample('D').mean()\n",
    "    \n",
    "    efficiency_metrics['daily_energy_mean'] = daily_energy.mean()\n",
    "    efficiency_metrics['daily_energy_std'] = daily_energy.std()\n",
    "    efficiency_metrics['daily_peak_mean'] = daily_peak.mean()\n",
    "    efficiency_metrics['daily_peak_std'] = daily_peak.std()\n",
    "    \n",
    "    # Peak demand analysis\n",
    "    # Count hours above different thresholds\n",
    "    thresholds = [0.5, 0.7, 0.8, 0.9]\n",
    "    peak_threshold = peak_power_w\n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        threshold_power = threshold * peak_threshold\n",
    "        hours_above = (consumption_hourly['total_power'] > threshold_power).sum()\n",
    "        efficiency_metrics[f'hours_above_{int(threshold*100)}pct_peak'] = hours_above\n",
    "        efficiency_metrics[f'pct_time_above_{int(threshold*100)}pct_peak'] = hours_above / total_period_hours * 100\n",
    "    \n",
    "    # Base load efficiency\n",
    "    if 'base_load_estimate' in locals():\n",
    "        base_load_ratio = base_load_estimate / avg_power_w\n",
    "        efficiency_metrics['base_load_ratio'] = base_load_ratio\n",
    "        efficiency_metrics['variable_load_ratio'] = 1 - base_load_ratio\n",
    "    \n",
    "    # Demand diversity (variability)\n",
    "    coefficient_of_variation = consumption_hourly['total_power'].std() / consumption_hourly['total_power'].mean()\n",
    "    efficiency_metrics['coefficient_of_variation'] = coefficient_of_variation\n",
    "    \n",
    "    # Seasonal efficiency (if data spans multiple months)\n",
    "    monthly_avg = consumption_hourly.groupby(consumption_hourly.index.month)['total_power'].mean()\n",
    "    if len(monthly_avg) > 1:\n",
    "        seasonal_variation = (monthly_avg.max() - monthly_avg.min()) / monthly_avg.mean()\n",
    "        efficiency_metrics['seasonal_variation'] = seasonal_variation\n",
    "    \n",
    "    # Time-of-use efficiency\n",
    "    peak_hours = consumption_hourly['is_working_hours']\n",
    "    peak_consumption = consumption_hourly[peak_hours]['total_power'].mean()\n",
    "    off_peak_consumption = consumption_hourly[~peak_hours]['total_power'].mean()\n",
    "    \n",
    "    if off_peak_consumption > 0:\n",
    "        peak_to_offpeak_ratio = peak_consumption / off_peak_consumption\n",
    "        efficiency_metrics['peak_to_offpeak_ratio'] = peak_to_offpeak_ratio\n",
    "    \n",
    "    # Display efficiency metrics\n",
    "    print(\"\\nEnergy Efficiency Analysis:\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nConsumption Summary:\")\n",
    "    print(f\"  Total energy: {efficiency_metrics['total_energy_kwh']:.1f} kWh\")\n",
    "    print(f\"  Average power: {efficiency_metrics['avg_power_w']:.0f} W\")\n",
    "    print(f\"  Peak power: {efficiency_metrics['peak_power_w']:.0f} W\")\n",
    "    print(f\"  Analysis period: {total_period_hours} hours ({total_period_hours/24:.1f} days)\")\n",
    "    \n",
    "    print(f\"\\nEfficiency Metrics:\")\n",
    "    print(f\"  Load factor: {efficiency_metrics['load_factor']:.3f}\")\n",
    "    print(f\"  Capacity factor: {efficiency_metrics['capacity_factor']:.3f}\")\n",
    "    print(f\"  Coefficient of variation: {efficiency_metrics['coefficient_of_variation']:.3f}\")\n",
    "    \n",
    "    if 'base_load_ratio' in efficiency_metrics:\n",
    "        print(f\"  Base load ratio: {efficiency_metrics['base_load_ratio']:.3f}\")\n",
    "        print(f\"  Variable load ratio: {efficiency_metrics['variable_load_ratio']:.3f}\")\n",
    "    \n",
    "    if 'peak_to_offpeak_ratio' in efficiency_metrics:\n",
    "        print(f\"  Peak/off-peak ratio: {efficiency_metrics['peak_to_offpeak_ratio']:.3f}\")\n",
    "    \n",
    "    if 'seasonal_variation' in efficiency_metrics:\n",
    "        print(f\"  Seasonal variation: {efficiency_metrics['seasonal_variation']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nPeak Demand Analysis:\")\n",
    "    for threshold in thresholds:\n",
    "        pct_time = efficiency_metrics[f'pct_time_above_{int(threshold*100)}pct_peak']\n",
    "        print(f\"  Time above {int(threshold*100)}% of peak: {pct_time:.1f}%\")\n",
    "    \n",
    "    # Efficiency benchmarking\n",
    "    print(f\"\\nEfficiency Benchmarking:\")\n",
    "    print(\"-\" * 30)\n",
    "    \n",
    "    # Load factor benchmarks\n",
    "    if load_factor > 0.8:\n",
    "        load_rating = \"Excellent (very stable load)\"\n",
    "    elif load_factor > 0.6:\n",
    "        load_rating = \"Good (moderately stable)\"\n",
    "    elif load_factor > 0.4:\n",
    "        load_rating = \"Fair (variable load)\"\n",
    "    else:\n",
    "        load_rating = \"Poor (highly variable)\"\n",
    "    \n",
    "    print(f\"Load factor rating: {load_rating}\")\n",
    "    \n",
    "    # Base load benchmarks (if available)\n",
    "    if 'base_load_ratio' in efficiency_metrics:\n",
    "        base_ratio = efficiency_metrics['base_load_ratio']\n",
    "        if base_ratio > 0.7:\n",
    "            base_rating = \"High base load (check for inefficiencies)\"\n",
    "        elif base_ratio > 0.4:\n",
    "            base_rating = \"Moderate base load (typical)\"\n",
    "        else:\n",
    "            base_rating = \"Low base load (efficient variable loads)\"\n",
    "        \n",
    "        print(f\"Base load rating: {base_rating}\")\n",
    "    \n",
    "    # Variability benchmarks\n",
    "    cv = coefficient_of_variation\n",
    "    if cv < 0.3:\n",
    "        var_rating = \"Low variability (stable consumption)\"\n",
    "    elif cv < 0.6:\n",
    "        var_rating = \"Moderate variability (typical)\"\n",
    "    else:\n",
    "        var_rating = \"High variability (check for anomalies)\"\n",
    "    \n",
    "    print(f\"Variability rating: {var_rating}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No consumption data available for efficiency analysis\")\n",
    "    efficiency_metrics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary and Recommendations\n",
    "\n",
    "Generate actionable insights and load optimization recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBase Load Analysis - Key Insights and Recommendations:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Generate insights based on analysis\n",
    "insights = []\n",
    "recommendations = []\n",
    "\n",
    "# Base load insights\n",
    "if 'base_load_estimate' in locals():\n",
    "    base_pct = base_load_estimate / consumption_hourly['total_power'].mean() * 100\n",
    "    insights.append(f\"Base load: {base_load_estimate:.0f}W ({base_pct:.1f}% of average consumption)\")\n",
    "    \n",
    "    if base_pct > 60:\n",
    "        recommendations.append(\"High base load detected - audit always-on devices for efficiency\")\n",
    "    elif base_pct < 30:\n",
    "        recommendations.append(\"Low base load indicates good energy management\")\n",
    "\n",
    "# Load pattern insights\n",
    "if 'daily_patterns' in locals():\n",
    "    weekday_peak = daily_patterns['weekday']['mean'].max()\n",
    "    weekend_peak = daily_patterns['weekend']['mean'].max()\n",
    "    peak_diff = abs(weekday_peak - weekend_peak) / weekday_peak * 100\n",
    "    \n",
    "    insights.append(f\"Peak consumption difference (weekday vs weekend): {peak_diff:.1f}%\")\n",
    "    \n",
    "    if peak_diff > 30:\n",
    "        recommendations.append(\"Significant weekday/weekend pattern - optimize for different schedules\")\n",
    "\n",
    "# Model performance insights\n",
    "if model_results:\n",
    "    best_model = max(model_results.keys(), key=lambda x: model_results[x]['r2_mean'])\n",
    "    best_r2 = model_results[best_model]['r2_mean']\n",
    "    best_mae = model_results[best_model]['mae_mean']\n",
    "    \n",
    "    insights.append(f\"Best forecasting model: {best_model} (R²={best_r2:.3f}, MAE={best_mae:.0f}W)\")\n",
    "    \n",
    "    if best_r2 > 0.8:\n",
    "        recommendations.append(\"High-quality load forecasting possible - implement predictive control\")\n",
    "    elif best_r2 < 0.6:\n",
    "        recommendations.append(\"Load forecasting challenging - investigate additional features\")\n",
    "\n",
    "# Anomaly insights\n",
    "if anomaly_results:\n",
    "    anomaly_rate = anomaly_results['anomaly_rate']\n",
    "    insights.append(f\"Anomaly rate: {anomaly_rate:.2f}% of consumption data\")\n",
    "    \n",
    "    if anomaly_rate > 5:\n",
    "        recommendations.append(\"High anomaly rate detected - investigate consumption irregularities\")\n",
    "    elif anomaly_rate < 1:\n",
    "        recommendations.append(\"Low anomaly rate indicates stable consumption patterns\")\n",
    "\n",
    "# Clustering insights\n",
    "if clustering_results:\n",
    "    n_clusters = clustering_results['optimal_clusters']\n",
    "    insights.append(f\"Load patterns clustered into {n_clusters} distinct daily profiles\")\n",
    "    \n",
    "    cluster_analysis = clustering_results['cluster_analysis']\n",
    "    weekday_clusters = sum(1 for analysis in cluster_analysis.values() \n",
    "                          if analysis['weekdays'] / analysis['count'] > 0.8)\n",
    "    \n",
    "    if weekday_clusters > 0:\n",
    "        recommendations.append(f\"Implement weekday-specific load management strategies\")\n",
    "    \n",
    "    recommendations.append(\"Use identified load patterns for demand response optimization\")\n",
    "\n",
    "# Efficiency insights\n",
    "if efficiency_metrics:\n",
    "    load_factor = efficiency_metrics['load_factor']\n",
    "    insights.append(f\"Load factor: {load_factor:.3f}\")\n",
    "    \n",
    "    if load_factor < 0.5:\n",
    "        recommendations.append(\"Low load factor - consider load balancing and peak shaving\")\n",
    "    elif load_factor > 0.8:\n",
    "        recommendations.append(\"High load factor indicates efficient system utilization\")\n",
    "    \n",
    "    if 'peak_to_offpeak_ratio' in efficiency_metrics:\n",
    "        ratio = efficiency_metrics['peak_to_offpeak_ratio']\n",
    "        if ratio > 2:\n",
    "            recommendations.append(\"High peak/off-peak ratio - implement time-of-use optimization\")\n",
    "\n",
    "# Seasonal insights\n",
    "if 'seasonal_variation' in efficiency_metrics:\n",
    "    seasonal_var = efficiency_metrics['seasonal_variation']\n",
    "    insights.append(f\"Seasonal variation: {seasonal_var:.3f}\")\n",
    "    \n",
    "    if seasonal_var > 0.5:\n",
    "        recommendations.append(\"High seasonal variation - adapt control strategies by season\")\n",
    "\n",
    "# Display insights and recommendations\n",
    "print(\"\\nKey Insights:\")\n",
    "for i, insight in enumerate(insights, 1):\n",
    "    print(f\"{i}. {insight}\")\n",
    "\n",
    "print(\"\\nOptimization Recommendations:\")\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(\"-\" * 40)\n",
    "if efficiency_metrics:\n",
    "    print(f\"Total energy analyzed: {efficiency_metrics['total_energy_kwh']:.1f} kWh\")\n",
    "    print(f\"Analysis period: {total_period_hours/24:.1f} days\")\n",
    "    print(f\"Average daily consumption: {efficiency_metrics['daily_energy_mean']:.1f} kWh\")\n",
    "    print(f\"Peak power demand: {efficiency_metrics['peak_power_w']:.0f} W\")\n",
    "\n",
    "if 'base_load_estimate' in locals():\n",
    "    print(f\"Estimated base load: {base_load_estimate:.0f} W\")\n",
    "\n",
    "if model_results:\n",
    "    print(f\"Best forecast accuracy: {best_mae:.0f}W MAE\")\n",
    "\n",
    "if clustering_results:\n",
    "    print(f\"Load pattern clusters: {n_clusters}\")\n",
    "\n",
    "print(f\"\\nBase load analysis completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save base load analysis results\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Create comprehensive results dictionary\n",
    "base_load_analysis_results = {\n",
    "    'analysis_period': {'start': start_date, 'end': end_date},\n",
    "    'consumption_summary': {\n",
    "        'total_hours': len(consumption_hourly) if not consumption_hourly.empty else 0,\n",
    "        'base_load_estimate': base_load_estimate if 'base_load_estimate' in locals() else None,\n",
    "        'efficiency_metrics': efficiency_metrics\n",
    "    },\n",
    "    'daily_patterns': daily_patterns if 'daily_patterns' in locals() else {},\n",
    "    'model_results': model_results,\n",
    "    'anomaly_results': {k: v for k, v in anomaly_results.items() if k != 'combined_anomalies'},  # Exclude series\n",
    "    'clustering_results': {k: v for k, v in clustering_results.items() if k not in ['profile_matrix', 'complete_days']},  # Exclude large arrays\n",
    "    'insights': insights,\n",
    "    'recommendations': recommendations\n",
    "}\n",
    "\n",
    "# Add decomposition results if available\n",
    "if 'decomposition_results' in locals() and decomposition_results:\n",
    "    base_load_analysis_results['decomposition_summary'] = {\n",
    "        'trend_strength': decomposition_results['trend_strength'],\n",
    "        'seasonal_strength': decomposition_results['seasonal_strength'],\n",
    "        'anomalies_count': len(decomposition_results['anomalies'])\n",
    "    }\n",
    "\n",
    "# Save to files\n",
    "results_dir = Path('../../../data/processed')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save as pickle for programmatic use\n",
    "with open(results_dir / 'base_load_analysis_results.pkl', 'wb') as f:\n",
    "    pickle.dump(base_load_analysis_results, f)\n",
    "\n",
    "# Save forecasting features if available\n",
    "if not features_df.empty:\n",
    "    features_sample = features_df.head(1000)  # Save sample for reference\n",
    "    features_sample.to_csv(results_dir / 'load_forecasting_features_sample.csv')\n",
    "\n",
    "# Save efficiency metrics as CSV\n",
    "if efficiency_metrics:\n",
    "    efficiency_df = pd.DataFrame([efficiency_metrics])\n",
    "    efficiency_df.to_csv(results_dir / 'energy_efficiency_metrics.csv', index=False)\n",
    "\n",
    "# Save summary as text\n",
    "with open(results_dir / 'base_load_analysis_summary.txt', 'w') as f:\n",
    "    f.write(\"Base Load Analysis Summary\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(f\"Analysis Period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\\n\")\n",
    "    f.write(f\"Data Points: {len(consumption_hourly) if not consumption_hourly.empty else 0} hours\\n\\n\")\n",
    "    \n",
    "    f.write(\"Key Insights:\\n\")\n",
    "    for i, insight in enumerate(insights, 1):\n",
    "        f.write(f\"{i}. {insight}\\n\")\n",
    "    \n",
    "    f.write(\"\\nRecommendations:\\n\")\n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        f.write(f\"{i}. {rec}\\n\")\n",
    "    \n",
    "    if efficiency_metrics:\n",
    "        f.write(\"\\nEfficiency Metrics:\\n\")\n",
    "        f.write(f\"  Load Factor: {efficiency_metrics['load_factor']:.3f}\\n\")\n",
    "        f.write(f\"  Average Power: {efficiency_metrics['avg_power_w']:.0f} W\\n\")\n",
    "        f.write(f\"  Peak Power: {efficiency_metrics['peak_power_w']:.0f} W\\n\")\n",
    "        f.write(f\"  Daily Energy (avg): {efficiency_metrics['daily_energy_mean']:.1f} kWh\\n\")\n",
    "\n",
    "print(\"\\nBase load analysis results saved to:\")\n",
    "print(f\"  - {results_dir / 'base_load_analysis_results.pkl'}\")\n",
    "if efficiency_metrics:\n",
    "    print(f\"  - {results_dir / 'energy_efficiency_metrics.csv'}\")\n",
    "print(f\"  - {results_dir / 'base_load_analysis_summary.txt'}\")\n",
    "if not features_df.empty:\n",
    "    print(f\"  - {results_dir / 'load_forecasting_features_sample.csv'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}