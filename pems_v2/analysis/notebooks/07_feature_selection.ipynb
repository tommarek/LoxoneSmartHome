{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection and Engineering for PEMS Models\n",
    "\n",
    "This notebook focuses on feature engineering and selection for machine learning models in the Predictive Energy Management System (PEMS).\n",
    "\n",
    "## Key Analysis Areas:\n",
    "1. **Feature Engineering**: Time-based, weather-derived, and energy-related features\n",
    "2. **Feature Selection**: Statistical and ML-based feature importance\n",
    "3. **Correlation Analysis**: Feature relationships and multicollinearity\n",
    "4. **Target Variable Analysis**: PV production, heating demand, consumption patterns\n",
    "5. **Model Input Preparation**: Cleaned and optimized feature sets for ML models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, f_regression, mutual_info_regression,\n",
    "    RFE, RFECV\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.append(str(Path().absolute().parent.parent))\n",
    "\n",
    "from analysis.data_extraction import DataExtractor\n",
    "from analysis.feature_engineering import FeatureEngineering\n",
    "from config.settings import PEMSSettings\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize settings and data extractor\n",
    "settings = PEMSSettings()\n",
    "extractor = DataExtractor(settings)\n",
    "feature_eng = FeatureEngineering()\n",
    "\n",
    "# Define analysis period\n",
    "end_date = datetime.now()\n",
    "start_date = end_date - timedelta(days=365)  # 1 year\n",
    "\n",
    "print(f\"Analysis period: {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all relevant data\n",
    "try:\n",
    "    print(\"Extracting data...\")\n",
    "    weather_data = await extractor.extract_weather_data(start_date, end_date)\n",
    "    pv_data = await extractor.extract_pv_data(start_date, end_date)\n",
    "    room_data = await extractor.extract_room_data(start_date, end_date)\n",
    "    consumption_data = await extractor.extract_consumption_data(start_date, end_date)\n",
    "    relay_states = await extractor.extract_relay_states(start_date, end_date)\n",
    "    price_data = await extractor.extract_energy_prices(start_date, end_date)\n",
    "    \n",
    "    print(f\"Weather data: {weather_data.shape}\")\n",
    "    print(f\"PV data: {pv_data.shape}\")\n",
    "    print(f\"Room data: {len(room_data)} rooms\")\n",
    "    print(f\"Consumption data: {consumption_data.shape}\")\n",
    "    print(f\"Relay states: {len(relay_states)} relays\")\n",
    "    print(f\"Price data: {price_data.shape if price_data is not None else 'None'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {e}\")\n",
    "    # Load from saved files if extraction fails\n",
    "    try:\n",
    "        print(\"Loading from saved parquet files...\")\n",
    "        weather_data = extractor.load_from_parquet(\"weather_data\")\n",
    "        pv_data = extractor.load_from_parquet(\"pv_data\")\n",
    "        room_data = extractor.load_from_parquet(\"room_data\")\n",
    "        print(\"Data loaded successfully from saved files\")\n",
    "    except:\n",
    "        print(\"Failed to load saved data. Please run data extraction first.\")\n",
    "        # Create dummy data for demonstration\n",
    "        dates = pd.date_range(start_date, end_date, freq='15T')\n",
    "        weather_data = pd.DataFrame({\n",
    "            'temperature': np.random.normal(15, 10, len(dates)),\n",
    "            'humidity': np.random.normal(70, 15, len(dates)),\n",
    "            'solar_radiation': np.maximum(0, np.random.normal(200, 150, len(dates))),\n",
    "            'wind_speed': np.random.exponential(3, len(dates))\n",
    "        }, index=dates)\n",
    "        \n",
    "        pv_data = pd.DataFrame({\n",
    "            'InputPower': np.maximum(0, weather_data['solar_radiation'] * 10 + np.random.normal(0, 100, len(dates)))\n",
    "        }, index=dates)\n",
    "        \n",
    "        print(\"Using dummy data for demonstration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive feature set\n",
    "print(\"Creating feature set...\")\n",
    "\n",
    "# Start with aligned time index (hourly)\n",
    "feature_df = pd.DataFrame(index=pd.date_range(start_date, end_date, freq='1H'))\n",
    "\n",
    "# === TIME-BASED FEATURES ===\n",
    "print(\"Adding time-based features...\")\n",
    "feature_df['hour'] = feature_df.index.hour\n",
    "feature_df['day_of_week'] = feature_df.index.dayofweek\n",
    "feature_df['month'] = feature_df.index.month\n",
    "feature_df['quarter'] = feature_df.index.quarter\n",
    "feature_df['day_of_year'] = feature_df.index.dayofyear\n",
    "feature_df['week_of_year'] = feature_df.index.isocalendar().week\n",
    "\n",
    "# Cyclical encoding for periodic features\n",
    "feature_df['hour_sin'] = np.sin(2 * np.pi * feature_df['hour'] / 24)\n",
    "feature_df['hour_cos'] = np.cos(2 * np.pi * feature_df['hour'] / 24)\n",
    "feature_df['day_sin'] = np.sin(2 * np.pi * feature_df['day_of_week'] / 7)\n",
    "feature_df['day_cos'] = np.cos(2 * np.pi * feature_df['day_of_week'] / 7)\n",
    "feature_df['month_sin'] = np.sin(2 * np.pi * feature_df['month'] / 12)\n",
    "feature_df['month_cos'] = np.cos(2 * np.pi * feature_df['month'] / 12)\n",
    "feature_df['dayofyear_sin'] = np.sin(2 * np.pi * feature_df['day_of_year'] / 365)\n",
    "feature_df['dayofyear_cos'] = np.cos(2 * np.pi * feature_df['day_of_year'] / 365)\n",
    "\n",
    "# Weekend/weekday indicator\n",
    "feature_df['is_weekend'] = (feature_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Season indicator\n",
    "feature_df['season'] = feature_df['month'].map({\n",
    "    12: 0, 1: 0, 2: 0,  # Winter\n",
    "    3: 1, 4: 1, 5: 1,   # Spring\n",
    "    6: 2, 7: 2, 8: 2,   # Summer\n",
    "    9: 3, 10: 3, 11: 3  # Autumn\n",
    "})\n",
    "\n",
    "print(f\"Time features added: {len([c for c in feature_df.columns if any(x in c for x in ['hour', 'day', 'month', 'season', 'weekend'])])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === WEATHER FEATURES ===\n",
    "print(\"Adding weather features...\")\n",
    "\n",
    "if not weather_data.empty:\n",
    "    # Resample weather data to hourly\n",
    "    weather_hourly = weather_data.resample('1H').mean()\n",
    "    \n",
    "    # Basic weather features\n",
    "    weather_cols = ['temperature', 'humidity', 'pressure', 'wind_speed', \n",
    "                   'cloud_cover', 'solar_radiation', 'precipitation']\n",
    "    \n",
    "    for col in weather_cols:\n",
    "        if col in weather_hourly.columns:\n",
    "            feature_df[col] = weather_hourly[col]\n",
    "    \n",
    "    # Derived weather features\n",
    "    if 'temperature' in feature_df.columns:\n",
    "        feature_df['temp_squared'] = feature_df['temperature'] ** 2\n",
    "        feature_df['temp_cubed'] = feature_df['temperature'] ** 3\n",
    "        \n",
    "        # Heating/cooling degree days (base 18°C)\n",
    "        feature_df['heating_degree_hours'] = np.maximum(0, 18 - feature_df['temperature'])\n",
    "        feature_df['cooling_degree_hours'] = np.maximum(0, feature_df['temperature'] - 24)\n",
    "        \n",
    "        # Temperature ranges\n",
    "        feature_df['temp_very_cold'] = (feature_df['temperature'] < 0).astype(int)\n",
    "        feature_df['temp_cold'] = ((feature_df['temperature'] >= 0) & \n",
    "                                  (feature_df['temperature'] < 10)).astype(int)\n",
    "        feature_df['temp_mild'] = ((feature_df['temperature'] >= 10) & \n",
    "                                  (feature_df['temperature'] < 20)).astype(int)\n",
    "        feature_df['temp_warm'] = (feature_df['temperature'] >= 20).astype(int)\n",
    "    \n",
    "    # Wind chill factor\n",
    "    if 'temperature' in feature_df.columns and 'wind_speed' in feature_df.columns:\n",
    "        # Simplified wind chill calculation\n",
    "        feature_df['wind_chill'] = (13.12 + 0.6215 * feature_df['temperature'] - \n",
    "                                   11.37 * (feature_df['wind_speed'] * 3.6) ** 0.16 + \n",
    "                                   0.3965 * feature_df['temperature'] * \n",
    "                                   (feature_df['wind_speed'] * 3.6) ** 0.16)\n",
    "    \n",
    "    # Solar position approximation\n",
    "    if 'solar_radiation' in feature_df.columns:\n",
    "        feature_df['solar_elevation'] = np.maximum(0, \n",
    "            np.sin(2 * np.pi * (feature_df['day_of_year'] - 81) / 365) * \n",
    "            np.sin(2 * np.pi * (feature_df['hour'] - 12) / 24) * 0.4)\n",
    "        \n",
    "        # Clear sky index (actual vs theoretical maximum)\n",
    "        theoretical_max = feature_df['solar_elevation'] * 1000  # W/m²\n",
    "        feature_df['clear_sky_index'] = np.where(theoretical_max > 0, \n",
    "                                                feature_df['solar_radiation'] / theoretical_max, 0)\n",
    "        feature_df['clear_sky_index'] = np.clip(feature_df['clear_sky_index'], 0, 1.2)\n",
    "    \n",
    "    print(f\"Weather features added: {len([c for c in feature_df.columns if any(x in c for x in ['temp', 'humid', 'wind', 'solar', 'cloud', 'precip', 'pressure'])])}\")\n",
    "else:\n",
    "    print(\"No weather data available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === ENERGY FEATURES ===\n",
    "print(\"Adding energy features...\")\n",
    "\n",
    "# PV features\n",
    "if not pv_data.empty:\n",
    "    pv_hourly = pv_data.resample('1H').agg({\n",
    "        'InputPower': ['mean', 'max', 'std'],\n",
    "        'OutputPower': ['mean', 'max'] if 'OutputPower' in pv_data.columns else None\n",
    "    }).fillna(0)\n",
    "    \n",
    "    if ('InputPower', 'mean') in pv_hourly.columns:\n",
    "        feature_df['pv_power_mean'] = pv_hourly[('InputPower', 'mean')]\n",
    "        feature_df['pv_power_max'] = pv_hourly[('InputPower', 'max')]\n",
    "        feature_df['pv_power_std'] = pv_hourly[('InputPower', 'std')].fillna(0)\n",
    "        \n",
    "        # PV capacity factor\n",
    "        feature_df['pv_capacity_factor'] = feature_df['pv_power_mean'] / 10000  # Assuming 10kW system\n",
    "        \n",
    "        # PV production indicators\n",
    "        feature_df['pv_producing'] = (feature_df['pv_power_mean'] > 50).astype(int)\n",
    "        feature_df['pv_peak_production'] = (feature_df['pv_power_mean'] > 7000).astype(int)\n",
    "    \n",
    "    print(f\"PV features added: {len([c for c in feature_df.columns if 'pv' in c])}\")\n",
    "\n",
    "# Consumption features\n",
    "if not consumption_data.empty:\n",
    "    consumption_hourly = consumption_data.resample('1H').agg(['mean', 'max', 'std']).fillna(0)\n",
    "    \n",
    "    if len(consumption_hourly.columns) > 0:\n",
    "        feature_df['consumption_mean'] = consumption_hourly.iloc[:, 0]  # First column mean\n",
    "        if len(consumption_hourly.columns) > 1:\n",
    "            feature_df['consumption_max'] = consumption_hourly.iloc[:, 1]\n",
    "        if len(consumption_hourly.columns) > 2:\n",
    "            feature_df['consumption_std'] = consumption_hourly.iloc[:, 2]\n",
    "    \n",
    "    print(f\"Consumption features added: {len([c for c in feature_df.columns if 'consumption' in c])}\")\n",
    "\n",
    "# Relay/heating features\n",
    "if relay_states:\n",
    "    total_relays = pd.DataFrame()\n",
    "    for room, relay_data in relay_states.items():\n",
    "        if not relay_data.empty:\n",
    "            if total_relays.empty:\n",
    "                total_relays = relay_data.resample('1H').mean()\n",
    "                total_relays.columns = ['total_heating_demand']\n",
    "            else:\n",
    "                total_relays['total_heating_demand'] += relay_data.resample('1H').mean().iloc[:, 0]\n",
    "    \n",
    "    if not total_relays.empty:\n",
    "        feature_df['heating_demand'] = total_relays['total_heating_demand']\n",
    "        feature_df['heating_on'] = (feature_df['heating_demand'] > 0.1).astype(int)\n",
    "        feature_df['heating_high'] = (feature_df['heating_demand'] > 8).astype(int)  # More than half relays\n",
    "    \n",
    "    print(f\"Heating features added: {len([c for c in feature_df.columns if 'heating' in c])}\")\n",
    "\n",
    "# Price features\n",
    "if price_data is not None and not price_data.empty:\n",
    "    price_hourly = price_data.resample('1H').mean()\n",
    "    \n",
    "    if 'price' in price_hourly.columns:\n",
    "        feature_df['energy_price'] = price_hourly['price']\n",
    "        \n",
    "        # Price-based features\n",
    "        price_median = feature_df['energy_price'].median()\n",
    "        feature_df['price_high'] = (feature_df['energy_price'] > price_median * 1.5).astype(int)\n",
    "        feature_df['price_low'] = (feature_df['energy_price'] < price_median * 0.5).astype(int)\n",
    "        \n",
    "        # Price relative to daily average\n",
    "        daily_avg_price = feature_df['energy_price'].resample('D').mean()\n",
    "        feature_df['price_vs_daily_avg'] = (feature_df['energy_price'] / \n",
    "                                           feature_df.index.to_series().dt.date.map(daily_avg_price))\n",
    "    \n",
    "    print(f\"Price features added: {len([c for c in feature_df.columns if 'price' in c])}\")\n",
    "\n",
    "print(f\"\\nTotal features created: {len(feature_df.columns)}\")\n",
    "print(f\"Data points: {len(feature_df)}\")\n",
    "print(f\"Memory usage: {feature_df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lag and Rolling Window Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === LAG FEATURES ===\n",
    "print(\"Adding lag and rolling window features...\")\n",
    "\n",
    "# Define key variables for lag features\n",
    "lag_variables = ['temperature', 'solar_radiation', 'pv_power_mean', 'heating_demand']\n",
    "lag_periods = [1, 2, 3, 6, 12, 24]  # 1h, 2h, 3h, 6h, 12h, 24h\n",
    "\n",
    "for var in lag_variables:\n",
    "    if var in feature_df.columns:\n",
    "        # Lag features\n",
    "        for lag in lag_periods:\n",
    "            feature_df[f'{var}_lag_{lag}h'] = feature_df[var].shift(lag)\n",
    "        \n",
    "        # Rolling window features\n",
    "        for window in [3, 6, 12, 24]:\n",
    "            feature_df[f'{var}_roll_mean_{window}h'] = feature_df[var].rolling(window).mean()\n",
    "            feature_df[f'{var}_roll_std_{window}h'] = feature_df[var].rolling(window).std()\n",
    "            feature_df[f'{var}_roll_max_{window}h'] = feature_df[var].rolling(window).max()\n",
    "            feature_df[f'{var}_roll_min_{window}h'] = feature_df[var].rolling(window).min()\n",
    "\n",
    "# Weekly and daily patterns\n",
    "for var in ['temperature', 'pv_power_mean', 'heating_demand']:\n",
    "    if var in feature_df.columns:\n",
    "        # Same hour yesterday\n",
    "        feature_df[f'{var}_same_hour_yesterday'] = feature_df[var].shift(24)\n",
    "        \n",
    "        # Same hour last week\n",
    "        feature_df[f'{var}_same_hour_last_week'] = feature_df[var].shift(24 * 7)\n",
    "        \n",
    "        # Average for this hour of week (historical)\n",
    "        hour_week_avg = feature_df.groupby([feature_df.index.hour, feature_df.index.dayofweek])[var].expanding().mean()\n",
    "        feature_df[f'{var}_hour_week_avg'] = hour_week_avg.droplevel([0, 1])\n",
    "\n",
    "print(f\"Lag and rolling features added. Total features: {len(feature_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE QUALITY ASSESSMENT ===\n",
    "print(\"Assessing feature quality...\")\n",
    "\n",
    "# Remove features with too many missing values\n",
    "missing_threshold = 0.3  # 30% missing values\n",
    "missing_ratios = feature_df.isnull().sum() / len(feature_df)\n",
    "features_to_drop = missing_ratios[missing_ratios > missing_threshold].index.tolist()\n",
    "\n",
    "print(f\"Features with >30% missing values (to be dropped): {len(features_to_drop)}\")\n",
    "if features_to_drop:\n",
    "    print(f\"Dropping: {features_to_drop[:5]}{'...' if len(features_to_drop) > 5 else ''}\")\n",
    "    feature_df = feature_df.drop(columns=features_to_drop)\n",
    "\n",
    "# Remove features with zero variance\n",
    "numeric_features = feature_df.select_dtypes(include=[np.number]).columns\n",
    "zero_variance_features = []\n",
    "for col in numeric_features:\n",
    "    if feature_df[col].std() == 0:\n",
    "        zero_variance_features.append(col)\n",
    "\n",
    "print(f\"Zero variance features (to be dropped): {len(zero_variance_features)}\")\n",
    "if zero_variance_features:\n",
    "    feature_df = feature_df.drop(columns=zero_variance_features)\n",
    "\n",
    "# Feature correlation analysis\n",
    "correlation_matrix = feature_df[numeric_features].corr()\n",
    "\n",
    "# Find highly correlated feature pairs\n",
    "high_corr_pairs = []\n",
    "correlation_threshold = 0.95\n",
    "\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > correlation_threshold:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i],\n",
    "                correlation_matrix.columns[j],\n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"Highly correlated feature pairs (>0.95): {len(high_corr_pairs)}\")\n",
    "\n",
    "# Remove one feature from each highly correlated pair\n",
    "features_to_remove = set()\n",
    "for feat1, feat2, corr in high_corr_pairs:\n",
    "    # Keep the feature with fewer missing values\n",
    "    if feature_df[feat1].isnull().sum() > feature_df[feat2].isnull().sum():\n",
    "        features_to_remove.add(feat1)\n",
    "    else:\n",
    "        features_to_remove.add(feat2)\n",
    "\n",
    "if features_to_remove:\n",
    "    print(f\"Removing {len(features_to_remove)} highly correlated features\")\n",
    "    feature_df = feature_df.drop(columns=list(features_to_remove))\n",
    "\n",
    "# Final feature summary\n",
    "print(f\"\\nFinal feature set: {len(feature_df.columns)} features\")\n",
    "print(f\"Missing value summary:\")\n",
    "remaining_missing = feature_df.isnull().sum()\n",
    "if remaining_missing.sum() > 0:\n",
    "    print(remaining_missing[remaining_missing > 0].sort_values(ascending=False).head(10))\n",
    "else:\n",
    "    print(\"No missing values in final feature set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target Variable Definition and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === TARGET VARIABLE PREPARATION ===\n",
    "print(\"Preparing target variables for different prediction tasks...\")\n",
    "\n",
    "# Clean the feature dataframe\n",
    "clean_features = feature_df.dropna()\n",
    "\n",
    "# Define prediction targets\n",
    "targets = {}\n",
    "\n",
    "# 1. PV Production Prediction (next hour)\n",
    "if 'pv_power_mean' in clean_features.columns:\n",
    "    targets['pv_next_hour'] = clean_features['pv_power_mean'].shift(-1)\n",
    "    print(f\"PV prediction target: {targets['pv_next_hour'].notna().sum()} valid samples\")\n",
    "\n",
    "# 2. Heating Demand Prediction (next hour)\n",
    "if 'heating_demand' in clean_features.columns:\n",
    "    targets['heating_next_hour'] = clean_features['heating_demand'].shift(-1)\n",
    "    print(f\"Heating prediction target: {targets['heating_next_hour'].notna().sum()} valid samples\")\n",
    "\n",
    "# 3. Total Consumption Prediction\n",
    "if 'consumption_mean' in clean_features.columns:\n",
    "    targets['consumption_next_hour'] = clean_features['consumption_mean'].shift(-1)\n",
    "    print(f\"Consumption prediction target: {targets['consumption_next_hour'].notna().sum()} valid samples\")\n",
    "\n",
    "# 4. Net Energy Balance (PV - Consumption)\n",
    "if 'pv_power_mean' in clean_features.columns and 'consumption_mean' in clean_features.columns:\n",
    "    net_balance = clean_features['pv_power_mean'] - clean_features['consumption_mean']\n",
    "    targets['net_balance_next_hour'] = net_balance.shift(-1)\n",
    "    print(f\"Net balance prediction target: {targets['net_balance_next_hour'].notna().sum()} valid samples\")\n",
    "\n",
    "# Display target variable distributions\n",
    "if targets:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, (target_name, target_data) in enumerate(targets.items()):\n",
    "        if i < len(axes) and target_data.notna().sum() > 0:\n",
    "            valid_data = target_data.dropna()\n",
    "            \n",
    "            axes[i].hist(valid_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'{target_name.replace(\"_\", \" \").title()} Distribution')\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add statistics\n",
    "            stats_text = f\"Mean: {valid_data.mean():.1f}\\nStd: {valid_data.std():.1f}\\nSamples: {len(valid_data)}\"\n",
    "            axes[i].text(0.7, 0.7, stats_text, transform=axes[i].transAxes, \n",
    "                        bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(targets), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid target variables could be created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE SELECTION ===\n",
    "print(\"Performing feature selection analysis...\")\n",
    "\n",
    "# Choose primary target for feature selection\n",
    "primary_target_name = 'pv_next_hour' if 'pv_next_hour' in targets else list(targets.keys())[0] if targets else None\n",
    "\n",
    "if primary_target_name and targets[primary_target_name].notna().sum() > 100:\n",
    "    print(f\"Using '{primary_target_name}' as primary target for feature selection\")\n",
    "    \n",
    "    # Prepare data for feature selection\n",
    "    target = targets[primary_target_name]\n",
    "    \n",
    "    # Get features excluding target-related columns\n",
    "    feature_columns = [col for col in clean_features.columns \n",
    "                      if not any(exclude in col.lower() for exclude in ['target', 'next_hour', 'future'])]\n",
    "    \n",
    "    X = clean_features[feature_columns]\n",
    "    y = target\n",
    "    \n",
    "    # Create common valid index\n",
    "    valid_idx = X.index.intersection(y.dropna().index)\n",
    "    X_valid = X.loc[valid_idx].dropna()\n",
    "    y_valid = y.loc[X_valid.index]\n",
    "    \n",
    "    print(f\"Valid samples for feature selection: {len(X_valid)}\")\n",
    "    print(f\"Number of features to evaluate: {len(X_valid.columns)}\")\n",
    "    \n",
    "    if len(X_valid) > 100 and len(X_valid.columns) > 5:\n",
    "        # Split data for evaluation\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X_valid, y_valid, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        # === 1. Univariate Feature Selection ===\n",
    "        print(\"\\n1. Univariate Feature Selection (F-statistic)...\")\n",
    "        \n",
    "        selector_f = SelectKBest(score_func=f_regression, k=min(20, len(X_train.columns)))\n",
    "        X_train_f = selector_f.fit_transform(X_train_scaled, y_train)\n",
    "        \n",
    "        # Get feature scores\n",
    "        f_scores = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'f_score': selector_f.scores_,\n",
    "            'p_value': selector_f.pvalues_\n",
    "        }).sort_values('f_score', ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 features by F-statistic:\")\n",
    "        display(f_scores.head(10))\n",
    "        \n",
    "        # === 2. Mutual Information ===\n",
    "        print(\"\\n2. Mutual Information Feature Selection...\")\n",
    "        \n",
    "        # Use subset of data for faster computation\n",
    "        subset_size = min(5000, len(X_train))\n",
    "        subset_idx = np.random.choice(len(X_train), subset_size, replace=False)\n",
    "        \n",
    "        mi_scores = mutual_info_regression(\n",
    "            X_train_scaled[subset_idx], y_train.iloc[subset_idx], \n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'mutual_info': mi_scores\n",
    "        }).sort_values('mutual_info', ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 features by Mutual Information:\")\n",
    "        display(mi_df.head(10))\n",
    "        \n",
    "        # === 3. Random Forest Feature Importance ===\n",
    "        print(\"\\n3. Random Forest Feature Importance...\")\n",
    "        \n",
    "        rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "        \n",
    "        rf_importance = pd.DataFrame({\n",
    "            'feature': X_train.columns,\n",
    "            'importance': rf.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"Top 10 features by Random Forest Importance:\")\n",
    "        display(rf_importance.head(10))\n",
    "        \n",
    "        # Model performance with all features\n",
    "        y_pred_rf = rf.predict(X_test_scaled)\n",
    "        rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "        rf_rmse = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "        \n",
    "        print(f\"\\nRandom Forest Performance (all features):\")\n",
    "        print(f\"R² Score: {rf_r2:.4f}\")\n",
    "        print(f\"RMSE: {rf_rmse:.2f}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"Insufficient data for comprehensive feature selection\")\n",
    "        f_scores = mi_df = rf_importance = None\n",
    "        \n",
    "else:\n",
    "    print(\"No suitable target variable available for feature selection\")\n",
    "    f_scores = mi_df = rf_importance = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE IMPORTANCE VISUALIZATION ===\n",
    "if f_scores is not None and mi_df is not None and rf_importance is not None:\n",
    "    print(\"Creating feature importance visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "    \n",
    "    # 1. F-statistic scores\n",
    "    top_f = f_scores.head(15)\n",
    "    axes[0, 0].barh(range(len(top_f)), top_f['f_score'], alpha=0.7)\n",
    "    axes[0, 0].set_yticks(range(len(top_f)))\n",
    "    axes[0, 0].set_yticklabels(top_f['feature'], fontsize=8)\n",
    "    axes[0, 0].set_xlabel('F-Statistic Score')\n",
    "    axes[0, 0].set_title('Top 15 Features by F-Statistic')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Mutual Information scores\n",
    "    top_mi = mi_df.head(15)\n",
    "    axes[0, 1].barh(range(len(top_mi)), top_mi['mutual_info'], alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_yticks(range(len(top_mi)))\n",
    "    axes[0, 1].set_yticklabels(top_mi['feature'], fontsize=8)\n",
    "    axes[0, 1].set_xlabel('Mutual Information Score')\n",
    "    axes[0, 1].set_title('Top 15 Features by Mutual Information')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Random Forest importance\n",
    "    top_rf = rf_importance.head(15)\n",
    "    axes[1, 0].barh(range(len(top_rf)), top_rf['importance'], alpha=0.7, color='green')\n",
    "    axes[1, 0].set_yticks(range(len(top_rf)))\n",
    "    axes[1, 0].set_yticklabels(top_rf['feature'], fontsize=8)\n",
    "    axes[1, 0].set_xlabel('Random Forest Importance')\n",
    "    axes[1, 0].set_title('Top 15 Features by Random Forest')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. Combined ranking\n",
    "    # Normalize scores and combine\n",
    "    combined_ranking = pd.DataFrame({'feature': X_train.columns})\n",
    "    \n",
    "    # Add normalized ranks (lower rank = better)\n",
    "    f_ranks = f_scores.reset_index()[['feature']].merge(\n",
    "        f_scores.reset_index()[['feature']].reset_index().rename(columns={'index': 'f_rank'}),\n",
    "        on='feature'\n",
    "    )\n",
    "    \n",
    "    mi_ranks = mi_df.reset_index()[['feature']].merge(\n",
    "        mi_df.reset_index()[['feature']].reset_index().rename(columns={'index': 'mi_rank'}),\n",
    "        on='feature'\n",
    "    )\n",
    "    \n",
    "    rf_ranks = rf_importance.reset_index()[['feature']].merge(\n",
    "        rf_importance.reset_index()[['feature']].reset_index().rename(columns={'index': 'rf_rank'}),\n",
    "        on='feature'\n",
    "    )\n",
    "    \n",
    "    # Merge all rankings\n",
    "    combined = combined_ranking.merge(f_ranks, on='feature', how='left')\n",
    "    combined = combined.merge(mi_ranks, on='feature', how='left')\n",
    "    combined = combined.merge(rf_ranks, on='feature', how='left')\n",
    "    \n",
    "    # Fill missing ranks with max rank + 1\n",
    "    max_rank = len(combined)\n",
    "    combined = combined.fillna(max_rank)\n",
    "    \n",
    "    # Calculate average rank\n",
    "    combined['avg_rank'] = (combined['f_rank'] + combined['mi_rank'] + combined['rf_rank']) / 3\n",
    "    combined = combined.sort_values('avg_rank')\n",
    "    \n",
    "    top_combined = combined.head(15)\n",
    "    axes[1, 1].barh(range(len(top_combined)), 1 / (top_combined['avg_rank'] + 1), alpha=0.7, color='red')\n",
    "    axes[1, 1].set_yticks(range(len(top_combined)))\n",
    "    axes[1, 1].set_yticklabels(top_combined['feature'], fontsize=8)\n",
    "    axes[1, 1].set_xlabel('Combined Importance (1/avg_rank)')\n",
    "    axes[1, 1].set_title('Top 15 Features by Combined Ranking')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print top features summary\n",
    "    print(\"\\n=== TOP 10 FEATURES BY COMBINED RANKING ===\")\n",
    "    top_10_combined = combined.head(10)\n",
    "    for i, row in top_10_combined.iterrows():\n",
    "        print(f\"{int(row.name)+1:2d}. {row['feature']:<35} (avg rank: {row['avg_rank']:.1f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"Feature importance visualization skipped - insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Optimal Feature Set Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === OPTIMAL FEATURE SET SELECTION ===\n",
    "if 'combined' in locals() and len(X_valid) > 100:\n",
    "    print(\"Finding optimal feature set size...\")\n",
    "    \n",
    "    # Test different feature set sizes\n",
    "    feature_counts = [5, 10, 15, 20, 30, 50, min(100, len(combined))]\n",
    "    performance_results = []\n",
    "    \n",
    "    for n_features in feature_counts:\n",
    "        if n_features <= len(combined):\n",
    "            # Select top N features\n",
    "            top_features = combined.head(n_features)['feature'].tolist()\n",
    "            \n",
    "            # Train model with selected features\n",
    "            X_train_selected = X_train[top_features]\n",
    "            X_test_selected = X_test[top_features]\n",
    "            \n",
    "            # Scale selected features\n",
    "            scaler_selected = StandardScaler()\n",
    "            X_train_scaled_selected = scaler_selected.fit_transform(X_train_selected)\n",
    "            X_test_scaled_selected = scaler_selected.transform(X_test_selected)\n",
    "            \n",
    "            # Train Random Forest\n",
    "            rf_selected = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "            rf_selected.fit(X_train_scaled_selected, y_train)\n",
    "            \n",
    "            # Evaluate\n",
    "            y_pred_selected = rf_selected.predict(X_test_scaled_selected)\n",
    "            r2_selected = r2_score(y_test, y_pred_selected)\n",
    "            rmse_selected = np.sqrt(mean_squared_error(y_test, y_pred_selected))\n",
    "            \n",
    "            performance_results.append({\n",
    "                'n_features': n_features,\n",
    "                'r2_score': r2_selected,\n",
    "                'rmse': rmse_selected\n",
    "            })\n",
    "            \n",
    "            print(f\"Features: {n_features:3d}, R²: {r2_selected:.4f}, RMSE: {rmse_selected:.2f}\")\n",
    "    \n",
    "    # Plot performance vs number of features\n",
    "    if performance_results:\n",
    "        perf_df = pd.DataFrame(performance_results)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # R² Score\n",
    "        axes[0].plot(perf_df['n_features'], perf_df['r2_score'], 'o-', linewidth=2, markersize=8)\n",
    "        axes[0].set_xlabel('Number of Features')\n",
    "        axes[0].set_ylabel('R² Score')\n",
    "        axes[0].set_title('Model Performance vs Feature Count')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Find optimal point (elbow in performance)\n",
    "        best_idx = perf_df['r2_score'].idxmax()\n",
    "        axes[0].axvline(perf_df.loc[best_idx, 'n_features'], color='red', linestyle='--', \n",
    "                       label=f'Best: {perf_df.loc[best_idx, \"n_features\"]} features')\n",
    "        axes[0].legend()\n",
    "        \n",
    "        # RMSE\n",
    "        axes[1].plot(perf_df['n_features'], perf_df['rmse'], 'o-', linewidth=2, markersize=8, color='orange')\n",
    "        axes[1].set_xlabel('Number of Features')\n",
    "        axes[1].set_ylabel('RMSE')\n",
    "        axes[1].set_title('Model Error vs Feature Count')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Recommend optimal feature set\n",
    "        optimal_n_features = perf_df.loc[best_idx, 'n_features']\n",
    "        optimal_r2 = perf_df.loc[best_idx, 'r2_score']\n",
    "        \n",
    "        print(f\"\\n=== OPTIMAL FEATURE SET RECOMMENDATION ===\")\n",
    "        print(f\"Recommended number of features: {optimal_n_features}\")\n",
    "        print(f\"Expected R² performance: {optimal_r2:.4f}\")\n",
    "        \n",
    "        # Show optimal feature list\n",
    "        optimal_features = combined.head(optimal_n_features)['feature'].tolist()\n",
    "        print(f\"\\nOptimal feature list:\")\n",
    "        for i, feature in enumerate(optimal_features, 1):\n",
    "            print(f\"{i:2d}. {feature}\")\n",
    "        \n",
    "        # Save optimal feature set\n",
    "        optimal_feature_set = {\n",
    "            'target_variable': primary_target_name,\n",
    "            'n_features': optimal_n_features,\n",
    "            'expected_r2': optimal_r2,\n",
    "            'features': optimal_features,\n",
    "            'selection_method': 'combined_ranking',\n",
    "            'created_date': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nOptimal feature set ready for model training!\")\n",
    "        \n",
    "else:\n",
    "    print(\"Optimal feature set selection skipped - insufficient data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Categories and Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === FEATURE CATEGORIZATION AND SUMMARY ===\n",
    "print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "\n",
    "# Categorize features\n",
    "feature_categories = {\n",
    "    'Time-based': [col for col in feature_df.columns if any(x in col for x in ['hour', 'day', 'month', 'season', 'weekend', 'quarter'])],\n",
    "    'Weather': [col for col in feature_df.columns if any(x in col for x in ['temp', 'humid', 'wind', 'solar', 'cloud', 'precip', 'pressure'])],\n",
    "    'Energy': [col for col in feature_df.columns if any(x in col for x in ['pv', 'consumption', 'heating', 'price', 'balance'])],\n",
    "    'Lag/Rolling': [col for col in feature_df.columns if any(x in col for x in ['lag', 'roll', 'yesterday', 'last_week', 'avg'])],\n",
    "    'Derived': [col for col in feature_df.columns if any(x in col for x in ['squared', 'cubed', 'degree', 'chill', 'index', 'factor'])]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    print(f\"{category:<15}: {len(features):3d} features\")\n",
    "    if len(features) > 0:\n",
    "        example_features = features[:3]\n",
    "        print(f\"                 Examples: {', '.join(example_features)}{'...' if len(features) > 3 else ''}\")\n",
    "\n",
    "# Feature quality metrics\n",
    "print(f\"\\nFeature quality metrics:\")\n",
    "print(f\"Total features created: {len(feature_df.columns)}\")\n",
    "print(f\"Features after cleaning: {len(clean_features.columns)}\")\n",
    "print(f\"Data completeness: {(1 - clean_features.isnull().sum().sum() / (len(clean_features) * len(clean_features.columns))):.1%}\")\n",
    "print(f\"Time span: {clean_features.index.min()} to {clean_features.index.max()}\")\n",
    "print(f\"Data points: {len(clean_features):,}\")\n",
    "\n",
    "# Engineering recommendations\n",
    "print(f\"\\n=== FEATURE ENGINEERING RECOMMENDATIONS ===\")\n",
    "recommendations = []\n",
    "\n",
    "if 'combined' in locals():\n",
    "    top_feature_types = []\n",
    "    for feature in combined.head(10)['feature']:\n",
    "        for category, features in feature_categories.items():\n",
    "            if feature in features:\n",
    "                top_feature_types.append(category)\n",
    "                break\n",
    "    \n",
    "    from collections import Counter\n",
    "    type_counts = Counter(top_feature_types)\n",
    "    most_important_type = type_counts.most_common(1)[0][0] if type_counts else \"Unknown\"\n",
    "    \n",
    "    recommendations.append(f\"Focus on {most_important_type.lower()} features - they dominate top rankings\")\n",
    "\n",
    "if len([col for col in feature_df.columns if 'lag' in col]) > 0:\n",
    "    recommendations.append(\"Lag features are valuable - consider expanding lag periods for seasonal patterns\")\n",
    "\n",
    "if len([col for col in feature_df.columns if any(x in col for x in ['sin', 'cos'])]) > 0:\n",
    "    recommendations.append(\"Cyclical encoding is effective - apply to more periodic variables\")\n",
    "\n",
    "if 'weather' in str(feature_categories).lower():\n",
    "    recommendations.append(\"Weather derivatives show value - consider more complex weather interactions\")\n",
    "\n",
    "recommendations.append(\"Regularly retrain feature selection with new data to adapt to changing patterns\")\n",
    "recommendations.append(\"Consider domain-specific features (heating curves, PV degradation, seasonal adjustments)\")\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i}. {rec}\")\n",
    "\n",
    "print(f\"\\nFeature engineering analysis completed successfully!\")\n",
    "print(f\"Ready for model training with optimized feature set.\")\n",
    "print(f\"Report generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}