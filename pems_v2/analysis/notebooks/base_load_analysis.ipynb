{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Base Load Analysis\n",
    "\n",
    "This notebook analyzes non-controllable load patterns in the smart home system.\n",
    "\n",
    "## Analysis Goals\n",
    "- Identify and characterize base load consumption patterns\n",
    "- Separate controllable from non-controllable loads\n",
    "- Analyze consumption by time of day and day of week\n",
    "- Detect anomalies and unusual consumption patterns\n",
    "- Optimize energy scheduling around base load patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from influxdb_client import InfluxDBClient\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "from scipy import signal, stats\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfluxDB connection parameters\n",
    "INFLUX_URL = \"http://localhost:8086\"\n",
    "INFLUX_TOKEN = \"your-token-here\"\n",
    "INFLUX_ORG = \"loxone\"\n",
    "INFLUX_BUCKET = \"loxone\"\n",
    "\n",
    "# Initialize InfluxDB client\n",
    "client = InfluxDBClient(url=INFLUX_URL, token=INFLUX_TOKEN, org=INFLUX_ORG)\n",
    "query_api = client.query_api()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_total_consumption(start_date, end_date):\n",
    "    \"\"\"Load total electrical consumption data\"\"\"\n",
    "    query = f'''\n",
    "    from(bucket: \"{INFLUX_BUCKET}\")\n",
    "        |> range(start: {start_date}, stop: {end_date})\n",
    "        |> filter(fn: (r) => r[\"_measurement\"] == \"power\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] =~ /total_consumption|grid_import/)\n",
    "        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)\n",
    "        |> yield(name: \"total_consumption\")\n",
    "    '''\n",
    "    result = query_api.query_data_frame(query)\n",
    "    return result\n",
    "\n",
    "def load_controllable_loads(start_date, end_date):\n",
    "    \"\"\"Load controllable load data (heating, cooling, water heater, etc.)\"\"\"\n",
    "    query = f'''\n",
    "    from(bucket: \"{INFLUX_BUCKET}\")\n",
    "        |> range(start: {start_date}, stop: {end_date})\n",
    "        |> filter(fn: (r) => r[\"_measurement\"] == \"power\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] =~ /heating|cooling|water_heater|heat_pump/)\n",
    "        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)\n",
    "        |> yield(name: \"controllable_loads\")\n",
    "    '''\n",
    "    result = query_api.query_data_frame(query)\n",
    "    return result\n",
    "\n",
    "def load_appliance_data(start_date, end_date):\n",
    "    \"\"\"Load individual appliance consumption data\"\"\"\n",
    "    query = f'''\n",
    "    from(bucket: \"{INFLUX_BUCKET}\")\n",
    "        |> range(start: {start_date}, stop: {end_date})\n",
    "        |> filter(fn: (r) => r[\"_measurement\"] == \"power\")\n",
    "        |> filter(fn: (r) => r[\"_field\"] =~ /lighting|appliances|fridge|washing_machine|dishwasher/)\n",
    "        |> aggregateWindow(every: 5m, fn: mean, createEmpty: false)\n",
    "        |> yield(name: \"appliances\")\n",
    "    '''\n",
    "    result = query_api.query_data_frame(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define analysis period\n",
    "end_date = datetime.now(pytz.UTC)\n",
    "start_date = end_date - timedelta(days=30)  # One month for pattern analysis\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading data from {start_date} to {end_date}\")\n",
    "total_consumption = load_total_consumption(start_date.isoformat(), end_date.isoformat())\n",
    "controllable_loads = load_controllable_loads(start_date.isoformat(), end_date.isoformat())\n",
    "appliance_data = load_appliance_data(start_date.isoformat(), end_date.isoformat())\n",
    "\n",
    "print(f\"Total consumption data shape: {total_consumption.shape}\")\n",
    "print(f\"Controllable loads data shape: {controllable_loads.shape}\")\n",
    "print(f\"Appliance data shape: {appliance_data.shape}\")\n",
    "\n",
    "# Get available fields\n",
    "if not total_consumption.empty:\n",
    "    total_fields = total_consumption['_field'].unique()\n",
    "    print(f\"\\nTotal consumption fields: {list(total_fields)}\")\n",
    "\n",
    "if not controllable_loads.empty:\n",
    "    controllable_fields = controllable_loads['_field'].unique()\n",
    "    print(f\"Controllable load fields: {list(controllable_fields)}\")\n",
    "\n",
    "if not appliance_data.empty:\n",
    "    appliance_fields = appliance_data['_field'].unique()\n",
    "    print(f\"Appliance fields: {list(appliance_fields)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Base Load Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate base load by subtracting controllable loads from total consumption\n",
    "if not total_consumption.empty and not controllable_loads.empty:\n",
    "    # Prepare data for merging\n",
    "    total_consumption['_time'] = pd.to_datetime(total_consumption['_time'])\n",
    "    controllable_loads['_time'] = pd.to_datetime(controllable_loads['_time'])\n",
    "    \n",
    "    # Aggregate total consumption\n",
    "    total_power = total_consumption.groupby('_time')['_value'].sum().reset_index()\n",
    "    total_power = total_power.rename(columns={'_value': 'total_power'})\n",
    "    \n",
    "    # Aggregate controllable loads\n",
    "    controllable_power = controllable_loads.groupby('_time')['_value'].sum().reset_index()\n",
    "    controllable_power = controllable_power.rename(columns={'_value': 'controllable_power'})\n",
    "    \n",
    "    # Merge and calculate base load\n",
    "    merged_data = pd.merge_asof(\n",
    "        total_power.sort_values('_time'),\n",
    "        controllable_power.sort_values('_time'),\n",
    "        on='_time',\n",
    "        direction='nearest',\n",
    "        tolerance=pd.Timedelta('5min')\n",
    "    )\n",
    "    \n",
    "    # Fill NaN with 0 for controllable power\n",
    "    merged_data['controllable_power'] = merged_data['controllable_power'].fillna(0)\n",
    "    \n",
    "    # Calculate base load\n",
    "    merged_data['base_load'] = merged_data['total_power'] - merged_data['controllable_power']\n",
    "    merged_data['base_load'] = merged_data['base_load'].clip(lower=0)  # Ensure non-negative\n",
    "    \n",
    "    # Add time features\n",
    "    merged_data['hour'] = merged_data['_time'].dt.hour\n",
    "    merged_data['weekday'] = merged_data['_time'].dt.day_name()\n",
    "    merged_data['is_weekend'] = merged_data['_time'].dt.weekday >= 5\n",
    "    merged_data['date'] = merged_data['_time'].dt.date\n",
    "    \n",
    "    print(f\"\\nBase Load Statistics:\")\n",
    "    print(f\"Average base load: {merged_data['base_load'].mean():.0f} W\")\n",
    "    print(f\"Minimum base load: {merged_data['base_load'].min():.0f} W\")\n",
    "    print(f\"Maximum base load: {merged_data['base_load'].max():.0f} W\")\n",
    "    print(f\"Standard deviation: {merged_data['base_load'].std():.0f} W\")\n",
    "    \n",
    "    # Plot load breakdown\n",
    "    fig, ax = plt.subplots(figsize=(14, 8))\n",
    "    \n",
    "    # Sample data for visualization (every 10th point)\n",
    "    sample_data = merged_data.iloc[::10]\n",
    "    \n",
    "    ax.fill_between(sample_data['_time'], 0, sample_data['base_load'], \n",
    "                   alpha=0.7, label='Base Load', color='lightblue')\n",
    "    ax.fill_between(sample_data['_time'], sample_data['base_load'], \n",
    "                   sample_data['base_load'] + sample_data['controllable_power'],\n",
    "                   alpha=0.7, label='Controllable Loads', color='orange')\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Power Consumption (W)')\n",
    "    ax.set_title('Power Consumption Breakdown: Base Load vs Controllable Loads')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Daily Base Load Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze daily base load patterns\n",
    "if 'base_load' in merged_data.columns:\n",
    "    # Calculate hourly patterns\n",
    "    hourly_base_load = merged_data.groupby(['hour', 'is_weekend'])['base_load'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Separate weekday and weekend patterns\n",
    "    weekday_pattern = hourly_base_load[~hourly_base_load['is_weekend']]\n",
    "    weekend_pattern = hourly_base_load[hourly_base_load['is_weekend']]\n",
    "    \n",
    "    # Plot daily patterns\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Hourly average pattern\n",
    "    ax1.plot(weekday_pattern['hour'], weekday_pattern['mean'], 'b-', \n",
    "            linewidth=2, label='Weekday', marker='o')\n",
    "    ax1.fill_between(weekday_pattern['hour'], \n",
    "                    weekday_pattern['mean'] - weekday_pattern['std'],\n",
    "                    weekday_pattern['mean'] + weekday_pattern['std'],\n",
    "                    alpha=0.3, color='blue')\n",
    "    \n",
    "    ax1.plot(weekend_pattern['hour'], weekend_pattern['mean'], 'r-', \n",
    "            linewidth=2, label='Weekend', marker='s')\n",
    "    ax1.fill_between(weekend_pattern['hour'], \n",
    "                    weekend_pattern['mean'] - weekend_pattern['std'],\n",
    "                    weekend_pattern['mean'] + weekend_pattern['std'],\n",
    "                    alpha=0.3, color='red')\n",
    "    \n",
    "    ax1.set_xlabel('Hour of Day')\n",
    "    ax1.set_ylabel('Base Load (W)')\n",
    "    ax1.set_title('Average Daily Base Load Pattern')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 23)\n",
    "    \n",
    "    # Box plot by hour\n",
    "    hourly_data = merged_data.groupby('hour')['base_load'].apply(list).reset_index()\n",
    "    box_data = [data for data in hourly_data['base_load']]\n",
    "    \n",
    "    ax2.boxplot(box_data, positions=range(24), patch_artist=True,\n",
    "               boxprops=dict(facecolor='lightgreen', alpha=0.7),\n",
    "               medianprops=dict(color='red', linewidth=2))\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Base Load (W)')\n",
    "    ax2.set_title('Base Load Distribution by Hour')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-0.5, 23.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify peak and minimum base load times\n",
    "    avg_hourly = merged_data.groupby('hour')['base_load'].mean()\n",
    "    peak_hour = avg_hourly.idxmax()\n",
    "    min_hour = avg_hourly.idxmin()\n",
    "    \n",
    "    print(f\"\\nDaily Pattern Analysis:\")\n",
    "    print(f\"Peak base load hour: {peak_hour}:00 ({avg_hourly[peak_hour]:.0f} W)\")\n",
    "    print(f\"Minimum base load hour: {min_hour}:00 ({avg_hourly[min_hour]:.0f} W)\")\n",
    "    print(f\"Peak to minimum ratio: {avg_hourly[peak_hour] / avg_hourly[min_hour]:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Weekly Base Load Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze weekly patterns\n",
    "if 'base_load' in merged_data.columns:\n",
    "    # Daily averages by weekday\n",
    "    daily_avg = merged_data.groupby('weekday')['base_load'].mean()\n",
    "    \n",
    "    # Order by weekday\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', \n",
    "                    'Friday', 'Saturday', 'Sunday']\n",
    "    daily_avg = daily_avg.reindex(weekday_order)\n",
    "    \n",
    "    # Plot weekly pattern\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Bar chart of daily averages\n",
    "    colors = ['blue' if day in ['Saturday', 'Sunday'] else 'orange' \n",
    "             for day in daily_avg.index]\n",
    "    bars = ax1.bar(range(len(daily_avg)), daily_avg.values, color=colors, alpha=0.7)\n",
    "    ax1.set_xlabel('Day of Week')\n",
    "    ax1.set_ylabel('Average Base Load (W)')\n",
    "    ax1.set_title('Average Base Load by Day of Week')\n",
    "    ax1.set_xticks(range(len(daily_avg)))\n",
    "    ax1.set_xticklabels([day[:3] for day in daily_avg.index], rotation=45)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, daily_avg.values):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 10,\n",
    "                f'{value:.0f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Heatmap of hourly patterns by weekday\n",
    "    pivot_data = merged_data.groupby(['weekday', 'hour'])['base_load'].mean().reset_index()\n",
    "    pivot_matrix = pivot_data.pivot(index='weekday', columns='hour', values='base_load')\n",
    "    pivot_matrix = pivot_matrix.reindex(weekday_order)\n",
    "    \n",
    "    im = ax2.imshow(pivot_matrix.values, cmap='viridis', aspect='auto')\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Day of Week')\n",
    "    ax2.set_title('Base Load Heatmap (W)')\n",
    "    ax2.set_yticks(range(len(weekday_order)))\n",
    "    ax2.set_yticklabels([day[:3] for day in weekday_order])\n",
    "    ax2.set_xticks(range(0, 24, 4))\n",
    "    ax2.set_xticklabels(range(0, 24, 4))\n",
    "    \n",
    "    # Add colorbar\n",
    "    cbar = plt.colorbar(im, ax=ax2)\n",
    "    cbar.set_label('Base Load (W)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print weekly statistics\n",
    "    print(f\"\\nWeekly Pattern Analysis:\")\n",
    "    print(f\"Highest consumption day: {daily_avg.idxmax()} ({daily_avg.max():.0f} W)\")\n",
    "    print(f\"Lowest consumption day: {daily_avg.idxmin()} ({daily_avg.min():.0f} W)\")\n",
    "    \n",
    "    weekend_avg = merged_data[merged_data['is_weekend']]['base_load'].mean()\n",
    "    weekday_avg = merged_data[~merged_data['is_weekend']]['base_load'].mean()\n",
    "    print(f\"\\nWeekend vs Weekday:\")\n",
    "    print(f\"Weekend average: {weekend_avg:.0f} W\")\n",
    "    print(f\"Weekday average: {weekday_avg:.0f} W\")\n",
    "    print(f\"Weekend/Weekday ratio: {weekend_avg / weekday_avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Base Load Clustering Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster daily base load patterns to identify different usage modes\n",
    "if 'base_load' in merged_data.columns:\n",
    "    # Create daily load profiles\n",
    "    daily_profiles = merged_data.groupby(['date', 'hour'])['base_load'].mean().reset_index()\n",
    "    daily_matrix = daily_profiles.pivot(index='date', columns='hour', values='base_load')\n",
    "    \n",
    "    # Remove days with missing data\n",
    "    daily_matrix = daily_matrix.dropna()\n",
    "    \n",
    "    if len(daily_matrix) > 5:  # Need at least 5 days for clustering\n",
    "        # Standardize the data\n",
    "        scaler = StandardScaler()\n",
    "        scaled_profiles = scaler.fit_transform(daily_matrix)\n",
    "        \n",
    "        # Perform K-means clustering\n",
    "        n_clusters = min(5, len(daily_matrix) // 3)  # Reasonable number of clusters\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "        clusters = kmeans.fit_predict(scaled_profiles)\n",
    "        \n",
    "        # Add cluster labels to data\n",
    "        daily_matrix['cluster'] = clusters\n",
    "        \n",
    "        # Plot cluster centers\n",
    "        fig, axes = plt.subplots(n_clusters, 1, figsize=(12, 3*n_clusters))\n",
    "        if n_clusters == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        colors = plt.cm.Set1(np.linspace(0, 1, n_clusters))\n",
    "        \n",
    "        for i in range(n_clusters):\n",
    "            cluster_data = daily_matrix[daily_matrix['cluster'] == i]\n",
    "            cluster_profiles = cluster_data.drop('cluster', axis=1)\n",
    "            \n",
    "            # Plot all profiles in cluster\n",
    "            for _, profile in cluster_profiles.iterrows():\n",
    "                axes[i].plot(range(24), profile.values, color=colors[i], alpha=0.3)\n",
    "            \n",
    "            # Plot cluster center\n",
    "            center = scaler.inverse_transform(kmeans.cluster_centers_[i].reshape(1, -1))[0]\n",
    "            axes[i].plot(range(24), center, color='black', linewidth=3, \n",
    "                        label=f'Cluster {i+1} Center')\n",
    "            \n",
    "            axes[i].set_xlabel('Hour of Day')\n",
    "            axes[i].set_ylabel('Base Load (W)')\n",
    "            axes[i].set_title(f'Cluster {i+1}: {len(cluster_data)} days')\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            axes[i].set_xlim(0, 23)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Analyze cluster characteristics\n",
    "        print(f\"\\nBase Load Clustering Analysis ({n_clusters} clusters):\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        cluster_stats = []\n",
    "        for i in range(n_clusters):\n",
    "            cluster_data = daily_matrix[daily_matrix['cluster'] == i]\n",
    "            cluster_profiles = cluster_data.drop('cluster', axis=1)\n",
    "            \n",
    "            avg_consumption = cluster_profiles.mean().mean()\n",
    "            peak_hour = cluster_profiles.mean().idxmax()\n",
    "            min_hour = cluster_profiles.mean().idxmin()\n",
    "            variability = cluster_profiles.mean().std()\n",
    "            \n",
    "            cluster_stats.append({\n",
    "                'cluster': i+1,\n",
    "                'days': len(cluster_data),\n",
    "                'avg_consumption': avg_consumption,\n",
    "                'peak_hour': peak_hour,\n",
    "                'min_hour': min_hour,\n",
    "                'variability': variability\n",
    "            })\n",
    "            \n",
    "            print(f\"Cluster {i+1}:\")\n",
    "            print(f\"  Days: {len(cluster_data)}\")\n",
    "            print(f\"  Average consumption: {avg_consumption:.0f} W\")\n",
    "            print(f\"  Peak hour: {peak_hour}:00\")\n",
    "            print(f\"  Minimum hour: {min_hour}:00\")\n",
    "            print(f\"  Daily variability: {variability:.0f} W\")\n",
    "            print()\n",
    "        \n",
    "        # Convert to DataFrame for easier analysis\n",
    "        cluster_df = pd.DataFrame(cluster_stats)\n",
    "        \n",
    "        # Identify cluster types\n",
    "        high_consumption_cluster = cluster_df.loc[cluster_df['avg_consumption'].idxmax()]\n",
    "        low_consumption_cluster = cluster_df.loc[cluster_df['avg_consumption'].idxmin()]\n",
    "        \n",
    "        print(f\"Pattern Insights:\")\n",
    "        print(f\"High consumption pattern: Cluster {high_consumption_cluster['cluster']} \"\n",
    "              f\"({high_consumption_cluster['avg_consumption']:.0f} W avg)\")\n",
    "        print(f\"Low consumption pattern: Cluster {low_consumption_cluster['cluster']} \"\n",
    "              f\"({low_consumption_cluster['avg_consumption']:.0f} W avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalous consumption patterns\n",
    "if 'base_load' in merged_data.columns:\n",
    "    # Calculate rolling statistics\n",
    "    merged_data = merged_data.sort_values('_time')\n",
    "    merged_data['rolling_mean'] = merged_data['base_load'].rolling(window=288).mean()  # 24-hour window (5min intervals)\n",
    "    merged_data['rolling_std'] = merged_data['base_load'].rolling(window=288).std()\n",
    "    \n",
    "    # Define anomalies as values beyond 3 standard deviations\n",
    "    merged_data['z_score'] = np.abs((merged_data['base_load'] - merged_data['rolling_mean']) / merged_data['rolling_std'])\n",
    "    merged_data['is_anomaly'] = merged_data['z_score'] > 3\n",
    "    \n",
    "    # Remove NaN values from rolling calculations\n",
    "    merged_data = merged_data.dropna(subset=['z_score'])\n",
    "    \n",
    "    anomalies = merged_data[merged_data['is_anomaly']]\n",
    "    \n",
    "    print(f\"\\nAnomaly Detection Results:\")\n",
    "    print(f\"Total data points: {len(merged_data)}\")\n",
    "    print(f\"Anomalies detected: {len(anomalies)} ({len(anomalies)/len(merged_data)*100:.2f}%)\")\n",
    "    \n",
    "    if len(anomalies) > 0:\n",
    "        print(f\"\\nAnomaly Statistics:\")\n",
    "        print(f\"Average anomaly value: {anomalies['base_load'].mean():.0f} W\")\n",
    "        print(f\"Maximum anomaly value: {anomalies['base_load'].max():.0f} W\")\n",
    "        print(f\"Minimum anomaly value: {anomalies['base_load'].min():.0f} W\")\n",
    "        \n",
    "        # Analyze anomaly timing\n",
    "        anomaly_hours = anomalies.groupby('hour').size()\n",
    "        if len(anomaly_hours) > 0:\n",
    "            print(f\"\\nMost common anomaly hours: {anomaly_hours.nlargest(3).to_dict()}\")\n",
    "    \n",
    "    # Plot anomalies\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Time series with anomalies highlighted\n",
    "    sample_data = merged_data.iloc[::50]  # Sample for better visualization\n",
    "    ax1.plot(sample_data['_time'], sample_data['base_load'], 'b-', alpha=0.7, label='Base Load')\n",
    "    ax1.plot(sample_data['_time'], sample_data['rolling_mean'], 'g-', linewidth=2, label='Rolling Mean')\n",
    "    \n",
    "    if len(anomalies) > 0:\n",
    "        anomaly_sample = anomalies.iloc[::10]  # Sample anomalies\n",
    "        ax1.scatter(anomaly_sample['_time'], anomaly_sample['base_load'], \n",
    "                   color='red', s=30, label='Anomalies', zorder=5)\n",
    "    \n",
    "    ax1.set_xlabel('Time')\n",
    "    ax1.set_ylabel('Base Load (W)')\n",
    "    ax1.set_title('Base Load with Anomaly Detection')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Distribution of Z-scores\n",
    "    ax2.hist(merged_data['z_score'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax2.axvline(x=3, color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "    ax2.set_xlabel('Z-Score')\n",
    "    ax2.set_ylabel('Frequency')\n",
    "    ax2.set_title('Distribution of Z-Scores for Base Load')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Load Forecasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple forecasting model for base load\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "if 'base_load' in merged_data.columns and len(merged_data) > 1000:\n",
    "    # Prepare features for forecasting\n",
    "    forecast_data = merged_data.copy()\n",
    "    \n",
    "    # Create lag features\n",
    "    for lag in [1, 12, 24, 288]:  # 5min, 1hr, 2hr, 24hr lags\n",
    "        forecast_data[f'base_load_lag_{lag}'] = forecast_data['base_load'].shift(lag)\n",
    "    \n",
    "    # Add time-based features\n",
    "    forecast_data['hour_sin'] = np.sin(2 * np.pi * forecast_data['hour'] / 24)\n",
    "    forecast_data['hour_cos'] = np.cos(2 * np.pi * forecast_data['hour'] / 24)\n",
    "    forecast_data['weekday_num'] = forecast_data['_time'].dt.weekday\n",
    "    forecast_data['is_weekend_num'] = forecast_data['is_weekend'].astype(int)\n",
    "    \n",
    "    # Rolling averages\n",
    "    forecast_data['base_load_ma_24'] = forecast_data['base_load'].rolling(window=288).mean()\n",
    "    forecast_data['base_load_ma_168'] = forecast_data['base_load'].rolling(window=2016).mean()  # 7 days\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = ['hour_sin', 'hour_cos', 'weekday_num', 'is_weekend_num',\n",
    "                   'base_load_lag_1', 'base_load_lag_12', 'base_load_lag_24', 'base_load_lag_288',\n",
    "                   'base_load_ma_24', 'base_load_ma_168']\n",
    "    \n",
    "    # Remove rows with NaN values\n",
    "    forecast_data = forecast_data.dropna(subset=feature_cols + ['base_load'])\n",
    "    \n",
    "    if len(forecast_data) > 500:\n",
    "        # Split data (80% train, 20% test)\n",
    "        split_idx = int(len(forecast_data) * 0.8)\n",
    "        train_data = forecast_data.iloc[:split_idx]\n",
    "        test_data = forecast_data.iloc[split_idx:]\n",
    "        \n",
    "        X_train = train_data[feature_cols]\n",
    "        y_train = train_data['base_load']\n",
    "        X_test = test_data[feature_cols]\n",
    "        y_test = test_data['base_load']\n",
    "        \n",
    "        # Train Random Forest model\n",
    "        rf_model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = rf_model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "        \n",
    "        print(f\"\\nBase Load Forecasting Model Performance:\")\n",
    "        print(f\"Mean Absolute Error: {mae:.2f} W\")\n",
    "        print(f\"Root Mean Square Error: {rmse:.2f} W\")\n",
    "        print(f\"Mean Absolute Percentage Error: {mape:.2f}%\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_cols,\n",
    "            'importance': rf_model.feature_importances_\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(f\"\\nTop 5 Most Important Features:\")\n",
    "        for _, row in feature_importance.head().iterrows():\n",
    "            print(f\"  {row['feature']}: {row['importance']:.3f}\")\n",
    "        \n",
    "        # Plot predictions vs actual\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "        \n",
    "        # Time series comparison (last 1000 points)\n",
    "        plot_data = test_data.tail(1000)\n",
    "        plot_pred = y_pred[-1000:]\n",
    "        \n",
    "        ax1.plot(plot_data['_time'], plot_data['base_load'], 'b-', \n",
    "                label='Actual', alpha=0.7, linewidth=1)\n",
    "        ax1.plot(plot_data['_time'], plot_pred, 'r-', \n",
    "                label='Predicted', alpha=0.7, linewidth=1)\n",
    "        ax1.set_xlabel('Time')\n",
    "        ax1.set_ylabel('Base Load (W)')\n",
    "        ax1.set_title('Base Load Forecast: Actual vs Predicted')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Scatter plot\n",
    "        ax2.scatter(y_test, y_pred, alpha=0.5, s=10)\n",
    "        ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], \n",
    "                'r--', linewidth=2, label='Perfect Prediction')\n",
    "        ax2.set_xlabel('Actual Base Load (W)')\n",
    "        ax2.set_ylabel('Predicted Base Load (W)')\n",
    "        ax2.set_title('Prediction Accuracy Scatter Plot')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Energy Scheduling Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize energy scheduling based on base load patterns\n",
    "if 'base_load' in merged_data.columns:\n",
    "    # Calculate optimal times for controllable loads\n",
    "    hourly_base_stats = merged_data.groupby('hour')['base_load'].agg(['mean', 'std']).reset_index()\n",
    "    \n",
    "    # Identify low base load hours (good for scheduling additional loads)\n",
    "    hourly_base_stats['load_score'] = hourly_base_stats['mean'] + hourly_base_stats['std']\n",
    "    hourly_base_stats = hourly_base_stats.sort_values('load_score')\n",
    "    \n",
    "    print(f\"\\nOptimal Scheduling Analysis:\")\n",
    "    print(f\"============================\")\n",
    "    \n",
    "    # Best hours for scheduling controllable loads\n",
    "    best_hours = hourly_base_stats.head(6)  # Top 6 hours\n",
    "    worst_hours = hourly_base_stats.tail(6)  # Bottom 6 hours\n",
    "    \n",
    "    print(f\"\\nBest hours for additional loads (lowest base + variability):\")\n",
    "    for _, row in best_hours.iterrows():\n",
    "        print(f\"  {row['hour']:02d}:00 - Avg: {row['mean']:.0f}W, Std: {row['std']:.0f}W\")\n",
    "    \n",
    "    print(f\"\\nAvoid scheduling during these hours (highest base + variability):\")\n",
    "    for _, row in worst_hours.iterrows():\n",
    "        print(f\"  {row['hour']:02d}:00 - Avg: {row['mean']:.0f}W, Std: {row['std']:.0f}W\")\n",
    "    \n",
    "    # Calculate potential load shifting benefits\n",
    "    avg_base_load = merged_data['base_load'].mean()\n",
    "    low_load_hours = hourly_base_stats.head(3)['mean'].mean()\n",
    "    high_load_hours = hourly_base_stats.tail(3)['mean'].mean()\n",
    "    \n",
    "    potential_reduction = high_load_hours - low_load_hours\n",
    "    \n",
    "    print(f\"\\nLoad Shifting Potential:\")\n",
    "    print(f\"Average base load: {avg_base_load:.0f} W\")\n",
    "    print(f\"Low-load periods average: {low_load_hours:.0f} W\")\n",
    "    print(f\"High-load periods average: {high_load_hours:.0f} W\")\n",
    "    print(f\"Potential peak reduction: {potential_reduction:.0f} W ({potential_reduction/avg_base_load*100:.1f}%)\")\n",
    "    \n",
    "    # Visualize optimization opportunities\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "    \n",
    "    # Base load with optimal scheduling windows\n",
    "    hours = hourly_base_stats['hour']\n",
    "    means = hourly_base_stats['mean']\n",
    "    \n",
    "    ax1.plot(hours, means, 'b-', linewidth=2, marker='o', label='Average Base Load')\n",
    "    ax1.fill_between(hours, means - hourly_base_stats['std'], \n",
    "                    means + hourly_base_stats['std'], alpha=0.3, color='blue')\n",
    "    \n",
    "    # Highlight optimal scheduling windows\n",
    "    optimal_hours = best_hours['hour'].values\n",
    "    for hour in optimal_hours:\n",
    "        ax1.axvspan(hour-0.5, hour+0.5, alpha=0.2, color='green')\n",
    "    \n",
    "    # Highlight avoid hours\n",
    "    avoid_hours = worst_hours['hour'].values\n",
    "    for hour in avoid_hours:\n",
    "        ax1.axvspan(hour-0.5, hour+0.5, alpha=0.2, color='red')\n",
    "    \n",
    "    ax1.set_xlabel('Hour of Day')\n",
    "    ax1.set_ylabel('Base Load (W)')\n",
    "    ax1.set_title('Base Load Pattern with Optimal Scheduling Windows')\n",
    "    ax1.legend(['Base Load', 'Optimal for Additional Loads', 'Avoid Additional Loads'])\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    ax1.set_xlim(0, 23)\n",
    "    \n",
    "    # Load variability analysis\n",
    "    ax2.bar(hours, hourly_base_stats['std'], color='orange', alpha=0.7)\n",
    "    ax2.set_xlabel('Hour of Day')\n",
    "    ax2.set_ylabel('Base Load Std Dev (W)')\n",
    "    ax2.set_title('Base Load Variability by Hour')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    ax2.set_xlim(-0.5, 23.5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Key Findings and Recommendations\n",
    "\n",
    "### Base Load Characteristics Summary\n",
    "\n",
    "Based on the base load analysis:\n",
    "\n",
    "1. **Consumption Patterns**\n",
    "   - Average base load and daily variation\n",
    "   - Peak and minimum consumption hours\n",
    "   - Weekend vs weekday differences\n",
    "   - Seasonal variations (if applicable)\n",
    "\n",
    "2. **Load Clusters**\n",
    "   - Different consumption patterns identified\n",
    "   - High vs low consumption days\n",
    "   - Pattern consistency and variability\n",
    "\n",
    "3. **Anomaly Detection**\n",
    "   - Unusual consumption events\n",
    "   - Potential equipment malfunctions\n",
    "   - Times of highest variability\n",
    "\n",
    "### Optimization Opportunities\n",
    "\n",
    "1. **Load Scheduling**\n",
    "   - Optimal hours for controllable loads\n",
    "   - Peak shaving opportunities\n",
    "   - Energy cost reduction potential\n",
    "\n",
    "2. **Demand Management**\n",
    "   - Identify high-consumption appliances\n",
    "   - Implement smart scheduling\n",
    "   - Battery storage optimization\n",
    "\n",
    "3. **Efficiency Improvements**\n",
    "   - Replace high base load appliances\n",
    "   - Implement standby power reduction\n",
    "   - Smart power strips and controls\n",
    "\n",
    "### Energy Savings Potential\n",
    "\n",
    "- Smart scheduling: 5-15% peak reduction\n",
    "- Standby power reduction: 10-20% base load reduction\n",
    "- Time-of-use optimization: 15-25% cost savings\n",
    "- Total potential: 20-40% improvement in energy efficiency\n",
    "\n",
    "### Implementation Recommendations\n",
    "\n",
    "1. **Immediate Actions**\n",
    "   - Schedule water heating during low base load hours\n",
    "   - Implement smart appliance controls\n",
    "   - Set up load monitoring alerts\n",
    "\n",
    "2. **Medium-term Improvements**\n",
    "   - Install smart meters for individual circuits\n",
    "   - Implement predictive load scheduling\n",
    "   - Optimize battery charging/discharging\n",
    "\n",
    "3. **Long-term Strategy**\n",
    "   - Replace inefficient appliances\n",
    "   - Implement demand response programs\n",
    "   - Integrate with dynamic electricity pricing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}